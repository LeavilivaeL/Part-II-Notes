\documentclass[10pt,a4paper]{article}
\input{./preamble.tex}

\title{Coding \& Cryptography}
\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Communication Channels}
This course will be about modelling communication. In general, we have the following idea:

\begin{figure}[H]
\centering
\begin{tikzpicture}
\node (s) at (0,0) {\textsc{Source}};
\node (e) at (4,0) {\textsc{Encoder}};
\node (d) at (8,0) {\textsc{Decoder}};
\node (r) at (12,0) {\textsc{Receiver}};
\draw (s) edge[->] (e) (d) edge[->] (r);
\draw[->, decorate, line join=round, decoration={zigzag, segment length=4, amplitude=0.9, post=lineto, post length=2pt}] (e) -- node[above] {\textsc{Channel}} node[below] {Errors, Noise} (d) ;
\end{tikzpicture}
\end{figure}
\vspace{-1em}
For example, the channel might be an optical or electrical telegraph, modems, audio CDs, satellite relays. The encoding and decoding might be something like \textsc{Ascii}, so that each character in the email ``Call at 2pm" would be encoded into 8 bits using $a = 1100101, \ldots$, giving an 84 bit message to be transmitted via the internet, and decoded by the receiver's email client. Our general aim here will be, given some source and channel (modelled probabilistically), to design an encoder and decoder to send messages economically and reliably.

\underline{Examples}
\begin{itemize}
\item (Noiseless coding) Morse Code. In this code, more common letters are assigned shorter codes, so that we have $A = \cdot - \ast, E = \cdot \ast, Q = - - \cdot - \ast, Z = - - \cdot \cdot \ast$. This is adapted to the \textit{source}, in the sense that we chose the codes based off the expected distribution of letters that we will have to transmit.

\item (Noisy coding) \textsc{Isbn}. In the \textsc{Isbn} encoding, every book is given a 10 digit number $a_1a_2\ldots a_{10}$, with $\sum_{i=1}^{10} (11-i)a_i \equiv 0 \mod 11$. This is adapted to the \textit{channel}, in the sense that the likely errors to occur will be 1 incorrect digit, or accidentally transposing two digits, which this code is resistant to (will return an error rather than an erroneous result).
\end{itemize}

A \emph{communication channel} accepts symbols from some alphabet $\mathscr{A} = \{a_1, a_2, \ldots, a_r\}$ (e.g. $\{0,1\}, \{a,b,\ldots,z\}$), and outputs symbols from an alphabet $\mathscr{B} = \{b_1, \ldots, b_s\}$. The channel is modelled by the probabilities:
\begin{center}$\P(y_1,y_2,\ldots,y_n$ received$| x_1,x_2,\ldots,x_n$ sent$) = \prod_{i=1}^{n} \P(y_i$ received$| x_i$ sent$)$\end{center}

A \emph{discrete memoryless channel (DMC)} is a channel with $p_{ij} = \P(b_j$ received$|a_i$ sent$)$ the same for each channel usage and independent of any past or future channel usages.

The \emph{channel matrix} is $P = (p_{ij})$, an $r \times s$ stochastic matrix.

\underline{Examples:} The \emph{binary symmetric channel (BSC)} with error probability $p \in [0,1]$ has $\mathscr{A} = \mathscr{B} = \{0,1\}$. The channel matrix is $\begin{pmatrix} 1-p & p \\ p & 1-p \end{pmatrix}$. A symbol is transmitted correctly with probability $1-p$.

The \emph{binary erasure channel} has input alphabet $\{0,1\}$, and output alphabet $\{0, 1, \ast\}$, where we miss a bit is probability $p$, giving channel matrix $\begin{pmatrix} 1-p & 0 & p \\ 0 & 1-p & 0\end{pmatrix}$. We can model $n$ uses of a channel by the \emph{n\th extension} with input alphabet $\mathscr{A}^n$, and output alphabet $\mathscr{B}^n$.

A \emph{code} $c$ of \emph{length} $n$ is a function $c: M \to \mathscr{A}^n$ where $M$ is the set of all possible messages. Implicitly, we also have a decoding rule $\mathscr{B}^n \to M$.\\
The \emph{size} of $c$ is $m = |M|$. \\
The \emph{information rate} is $\rho(c) = \frac{1}{n}\log_2(m)$. \\
The \emph{error rate} is $\hat{e}(c) = \max_{x\in M} \{\P($error$|x$ sent$)\}$.

A channel can transmit reliably at a rate $R$ if there exists a sequence of codes $(c_n: n\geq 1)$ with $c_n$ a code of length $n$, $\lim_{n\to \infty}(\rho(c_n)) =R, \lim_{n\to\infty}(\hat{e}(c_n)) = 0$. The capacity of a channel is the supremum of all reliable transmission rates.

\begin{theorem}
A $BSC$ with error probability $p < \frac12$ has a non-zero capacity (i.e. good codes exist).
\end{theorem}
\begin{proof}
See \textbf{9.3}
\end{proof}

\section{Noiseless Coding}
\subsection{Prefix-free Codes}
For an alphabet $\mathscr{A}, |\mathscr{A}|<\infty$, let $\mathscr{A}^{\ast} = \bigcup_{n\geq 0} \mathscr{A}^n$, the set of all finite strings from $\mathscr{A}$. The \emph{concatenation} of strings $x = x_1\ldots x_r$ and $y = y_1\ldots y_s$ is $xy = x_1\ldots x_ry_1\ldots y_s$.

Let $\mathscr{A}, \mathscr{B}$, be alphabets. A \emph{code} is a function $c:\mathscr{A} \to \mathscr{B}^{\ast}$. The strings $c(a)$ for $a \in \mathscr{A}$ are called \emph{codewords} (cws). If $x, y \in \mathscr{B}^{\ast}$ then $x$ is a \emph{prefix} of $y$ if $y=xz$ for some $z \in \mathscr{B}^{\ast}$.

For example, we have the Greek fire code, found in the writings of Polybius around 280 BC. $\mathscr{A} = \{\alpha, \beta, \ldots, \omega\}, \mathscr{B} = \{1,2,3,4,5\}$, with code $\alpha \mapsto 11, \beta \mapsto 12, \ldots, \psi \mapsto 53, \omega \mapsto, 54$, where $xy$ means ``$x$ torches held up, and another $y$ torches nearby".

The English language is even a code: we can let $\mathscr{A}$ be words in a given dictionary, and $\mathscr{B} = \{a,b,\ldots,z,\text{\textvisiblespace}\}$, where the coding function is to spell the word and follow it with a space.

We send a message $x_1\ldots x_n \in \mathscr{A}^{\ast}$ as $c(x_1)\ldots c(x_n) \in \mathscr{B}^{\ast}$. So $c$ extends to a function $c^{\ast}:\mathscr{A}^{\ast} \to \mathscr{B}^{\ast}$.

$c$ is \emph{decipherable}/\emph{decidable} if $c^{\ast}$ is injective, so that each string in $\mathscr{B}^{\ast}$ could have come from at most one message. Note that it isn't sufficient to just have $c$ injective, although clearly this is necessary:

$\mathscr{A} = \{1,2,3,4\}, \mathscr{B} = \{0,1\}, c:1\mapsto 0, 2\mapsto 1, 3\mapsto 00, 4\mapsto 01$. Then $c^{\ast}(114) = 0001 = c^{\ast}(312)$.

If $|\mathscr{A}|=m, |\mathscr{B}|=a$, then we say $c$ is an \emph{a-ary code of size m}. 2-ary = \emph{binary}, 3-ary=\emph{ternary}.

We aim to construct decipherable codes with short word lengths. Assuming $c$ is injective, the following are always decipherable:
\begin{itemize}
\item Block codes, where every codeword has the same length (e.g. Greek fire, \textsc{Ascii})
\item Comma codes, where we have an ``end of word" character (e.g. English language)
\item Prefix-free codes, where no codeword is a prefix of any other distinct words.
\end{itemize}
Note that both of the first two are special cases of prefix-free codes. Prefix-free codes are often called \emph{instantaneous} or \emph{self-punctuating} codes. Note that not all decipherable codes are prefix-free: $0\mapsto 01, 1 \mapsto 011$ is decipherable but not prefix free.

\begin{theorem}[Kraft's Inequality]
Let $|\mathscr{A}| = m, |\mathscr{B}| = a$. A prefix-free code $c:\mathscr{A} \to \mathscr{B}^{\ast}$ with word lengths $\ell_1, \ldots, \ell_m$ exists if and only if:
\begin{align*}
\sum_{i=1}^{m} a^{-\ell_i} &\leq 1 \tag{\ast}
\end{align*}
\end{theorem}
\begin{proof}
Rewrite (\ast) as $\sum_{\ell=1}^s n_{\ell}a^{-\ell} \leq 1 (\star)$, where $n_{\ell}$ is the number of codewords of length $\ell$ and $s = \max_{1 \leq i \leq m} \ell_i$.

\begin{itemize}
\item[$\implies$] If $c:\mathscr{A} \to \mathscr{B}^{\ast}$ is prefix-free, then $n_1 a^{s-1} + n_2 a^{s-2} + \ldots n_s \leq a^s$, since the \textsc{Lhs} is the number of strings of length $s$ in $\mathscr{B}$ with some codeword of $c$ as a prefix, and \textsc{Rhs} is the number of strings of length $s$. Dividing by $a^{s}$ gives $(\star)$.

\item[$\impliedby$] Given $n_1, \ldots, n_s$ satisfying $(\star)$, we need to construct a prefix-free code $c$ with $n_\ell$ codewords of length $\ell$ for all $\ell \leq s$. We use induction on $s$. The case $s=1$ is clear: we have $(\star)$ gives $n_1 \leq a$, so we can choose a code.

By the induction hypothesis there is a prefix-free code $\hat{c}$ with $n_{\ell}$ codewords of length $\ell$ for all $\ell \leq s-1$. Then $(\star)$ gives:
\begin{align*}
n_1 a^{s-1} + n_2 a^{s-2} + \ldots + n_{s-1}a + n_s &\leq a^s
\end{align*}
where the first $s-1$ terms on \textsc{Lhs} sum to the number of strings of length $s$ with some codeword of $\hat{c}$ as a prefix, and the \textsc{Rhs} is the number of strings of length $s$. Hence we can add at least $n_s$ new codewords of length $s$ to $\hat{c}$ and maintain the prefix-free property, giving our code.
\end{itemize}
\end{proof}

\begin{theorem}[McMillan]
Any decipherable code satisfies Kraft's inequality
\end{theorem}
\begin{proof}[Proof (Karush).]
Let $c:\mathscr{A} \to \mathscr{B}^{\ast}$ be a decipherable code with codewords of lengths $\ell_1, \ldots, \ell_m$. Let $s = \max_{1 \leq i\leq m} \ell_i$. Then for $R \in \N:$
\begin{align*}
\left(\sum_{i=1}^{m} a^{-\ell_i}\right)^R = \sum_{l=1}^{Rs} b_{\ell}a^{-\ell}
\end{align*}
where $b_{\ell} = |\{x \in \mathscr{A}^R : c^{\ast}(x)$ has length $\ell\}| \leq |\mathscr{B}^{\ell}| = a^{\ell}$, using the fact that $c^{\ast}$ is injective. Then:
\begin{align*}
\left(\sum_{i=1}^m a^{-\ell_i}\right)^R &\leq \sum_{l=1}^R a^\ell a^{-\ell} = Rs\\
\sum_{i=1}^m a^{-\ell_i} &\leq (Rs)^{\frac1R} \to 1 \text{ as } R \to \infty
\end{align*}
\end{proof}

\begin{corollary}
A decipherable code with prescribed word lengths exists iff a prefix-free code with same word lengths exists.
\end{corollary}
\begin{proof}
\item
\begin{itemize}
\item[$\implies$] Use \textbf{1.2} to generate a prefix-free code by \textbf{1.1}
\item[$\impliedby$] Prefix-free codes are decipherable.
\end{itemize}
\end{proof}

\section{Shannon's Noiseless Coding Theorem}
Entropy is a measure of `randomness' or `uncertainty'. Suppose we have a random variable $X$ that takes values $x_1, \ldots, x_n$ with probabilities $p_1, \ldots, p_n$. Then the \emph{entropy} (roughly speaking) is the expected number of fair coin tosses needed to simulate $X$.

\hspace*{-1em}\underline{Examples:}
\begin{itemize}
\item $p_1 = p_2 = p_3 = p_4 = \frac14$. We can identify $\{x_1, x_2, x_3, x_4\}$ with $\{HH, HT, TH, TT\}$, and so the entropy of this random variable is $2$.

\item $(p_1, p_2, p_3, p_4)=(\frac12,\frac14,\frac18,\frac18)$. Here, the entropy is $1\cdot\frac12+2\cdot\frac14+3\cdot\frac18+3\cdot\frac18 = \frac74$. We might say then, since the entropy is greater, that the first example is ``more random" than the second.
\end{itemize}

More concretely, the \emph{Shannon (or information) entropy} of $X$ is $H(X) = -\sum_{i=1}^n p_i \log_2 p_i$. Note that $H(X) \geq 0$ with equality if and only if $\P(X=x_i) = 1$. This is measured in \emph{bits}. We take the convention that $0\log 0 =0$.

\hspace*{-1em}\underline{Example}: Consider a biased coin, where $\P(X=H) = p, \P(X=T) = 1-p$. Then $H(X) = -p\log p-(1-p)\log(1-p) = p\log(\frac{1-p}{p} - \log(1-p)$.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[xmin=0, xmax=1 ,ymax=1.5, samples=50, xlabel = {$p$}, ylabel={$H(X)$}]
\addplot[blue, ultra thick, samples=200] (x, {x*log2((1-x)/x) - log2(1-x)});
\end{axis}
\end{tikzpicture}
\end{figure}

\begin{proposition}[Gibb's Inequality]
Let $(p_1, \ldots, p_n)$ and $(q_1, \ldots, q_n)$ be probability distributions. Then:
\begin{align*}
-\sum_{i=1}^n p_i \log p_i \leq -\sum_{i=1}^n p_i \log q_i
\end{align*}
with equality if and only if $p_i = q_i$ for all $i$.
\end{proposition}

\begin{proof}
Since $\log x = \frac{\ln x}{\ln 2}$, we may replace $\log$ by $\ln$ in the proof. Put $I = \{1 \leq r\leq n: p_i \neq 0\}$. Now $\ln x \leq x-1$ with equality if and only if $x=1$. So we have $\ln \frac{q_i}{p_i} \leq \frac{q_i}{p_i} - 1$, and hence:
\begin{align*}
\sum_{i \in I} p_i \ln \frac{q_i}{p_i} &\leq \sum_{i\in I}q_i - \sum_{i \in I}p_i\\
&= \sum_{i \in I} q_i - 1 \leq 0\\
\therefore - \sum_{i \in I}p_i \ln p_i &\leq - \sum_{i \in I} p_i \ln q_i\\
\therefore -\sum_{i=1}^n p_i \log p_i &\leq -\sum_{i=1}^n p_i \log q_i
\end{align*}
If equality holds, then $\sum_{i\in I} p_i =1$ and $\frac{p_i}{q_i} = 1$ for all $i \in I$, so $p_i = q_i$
\end{proof}

\begin{corollary}
$H(p_1, \ldots, p_n) \leq \log n$ with equality if and only if $p_1 = \ldots = p_n = \frac1n$.
\end{corollary}
\begin{proof}
Take $q_1 = \ldots = q_n = \frac1n$ in \textbf{2.1}.
\end{proof}

Let $\mathscr{A} = \{\mu_1, \ldots, \mu_m\}$, and $|\mathscr{B}| = a$, where $m, a\geq 2$. The random variable $X$ takes values $\mu_1, \ldots, \mu_m$ with probabilities $p_1, \ldots, p_m$. We say a code $c:\mathscr{A} \to \mathscr{B}^{\ast}$ is \emph{optimal} if it is a decipherable code with smallest possible expected word length, $\E S = \sum_i p_i \ell_i$.

\begin{theorem}[Shannon's Noiseless Coding Theorem]
The expected word length $\E S$ of an optimal code satisfies:
\begin{align*}
\frac{H(X)}{\log a} \leq \E S < \frac{H(X)}{\log a}+1
\end{align*}
\end{theorem}
\begin{proof}
For the lower bound, take $c: \mathscr{A} \to \mathscr{B}^{\ast}$ decipherable with word lengths $\ell_1, \ldots, \ell_m$. Then set $q_i = \frac{a^{-\ell_i}}{D}$ where $D = \sum_{i=1}^m a^{-\ell_i}$. Now we have that $\sum_{i=1}^m q_i = 1$. By Gibbs,
\begin{align*}
H(X) &\leq -\sum_{i=1}^m p_i \log q_i\\
&= -\sum p_i\left(-\ell_i \log a - \log D\right)\\
&= \left(\sum_{i=1}^m p_i \ell_i\right) \log a + \log D
\end{align*}
By McMillan, $D \leq 1$, so $\log D \leq 0$, and so $H(X) \leq \left(\sum_{i=1}^m p_i \ell_i\right) \log a = \E S \cdot \log a$, and we have equality if and only if $p_i = a^{-\ell_i}$ for some integers $\ell_1, \ldots \ell_m$.

For the upper bound, take $\ell_i = \ceil{-\log_a p_i}$. Then $-\log_a p_i \leq \ell_i \implies p_i \geq a^{-\ell_i}$.

Now $\sum_{i=1}^m a^{-\ell_i} \leq \sum_{i=1}^m p_i = 1$. By Kraft, there is some prefix-free code $c$ with word lengths $\ell_1, \ldots, \ell_m$, and the expected word length of $c$ is $\E S = \sum p_i \ell_i < \sum p_i (-\log_a p_i + 1) = \frac{H(X)}{\log a} + 1$.
\end{proof}

\hspace*{-1em}\underline{Example:} \emph{Shannon-Fano coding}\\
We mimic the above proof: given probabilities $p_1, \ldots, p_n$, set $\ell_i = \ceil{-\log_a p_i}$. Construct the prefix-free code with word lengths $\ell_1, \ldots, \ell_m$ by choosing in order of increasing length, ensuring that previous codewords are not prefixes. For example, if $a = 2, m = 5$ we have:\\
\begin{figure}[H]
\centering
\begin{tabular}{c|c|c|c}
$i$ & $p_i$ & $\ceil{-\log_2 p_i}$ & Codewords \\\hline
1 & 0.4 & 2 & 00 \\
2 & 0.2 & 3 & 010\\
3 & 0.2 & 3 & 011\\
4 & 0.1 & 4 & 1000\\
5 & 0.1 & 4 & 1001
\end{tabular}
\end{figure}
$\E S = \sum p_i \ell_i = 2.8$, entropy $= 2.12$

\section{Huffman Coding Algorithm}
Huffman was a student of Fano, and was thinking about how to construct an optimal code. For simplicity, we will take $a = 2$. Suppose we get messages with orders $p_1 \geq p_2 \geq \ldots \geq p_m$. Huffman gave a recursive definition of codes that we can prove are optimal. If $m=2$, then take codewords $0$ and $1$.

If $m > 2$, we first have a Huffman code for messages $\mu_1, \ldots, \mu_{m-2}, \nu$ with probabilities $p_1, \ldots, p_{m-2}, p_{m-1}+p_m$, then append $0$ and $1$ to give codewords for $\mu_{m-1}$ and $\mu_m$.

\hspace*{-1em}Note:
\begin{itemize}
\item Huffman codes are prefix-free.
\item We have some choices to make if some of the $p_j$ are equal, so Huffman codes are not unique.
\end{itemize}
\hspace*{-1em}\underline{Example:} Reconsider the previous example:
\begin{figure}[H]
\begin{subfigure}{0.5\textwidth}
\begin{tikzpicture}
\node[draw, circle] (a) at (0,10) {$p_i$};
\node[draw, circle] (b) at (-1,9) {0.6};
\node[draw, circle] (c) at (1,9) {0.4};
\node[draw, circle] (d) at (0,8) {0.4};
\node[draw, circle] (e) at (-2,8) {0.2};
\node[draw, circle] (f) at (1,7) {0.2};
\node[draw, circle] (g) at (-1,7) {0.2};
\node[draw, circle] (h) at (0,6) {0.1};
\node[draw, circle] (i) at (-2,6) {0.1};

\draw (a) -- node[above left] {0} (b) (a) -- node[above right] {1} (c);
\draw (b) -- node[above left] {0} (e) (b) -- node[above right] {1} (d);
\draw (d) -- node[above left] {0} (g) (d) -- node[above right] {1} (f);
\draw (g) -- node[above left] {0} (i) (g) -- node[above right] {1} (h);
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\begin{tabular}{c|c|c}
$i$ & $p_i$ & Codewords \\\hline
1 & 0.4 & 1\\
2 & 0.2 & 00\\
3 & 0.2 & 011\\
4 & 0.1 & 0100\\
5 & 0.1 & 0101
\end{tabular}
\end{subfigure}
\end{figure}
This code has expected length $2.2$, which is less than Shannon-Fano gave.
\begin{theorem}[Huffman, 1952]
Huffman codes are optimal.
\end{theorem}
\begin{proof}
We show this by induction on $m$. The case of $m=2$ is trivial. For $m > 2$, let $c_m$ be a Huffman code for source $X_m$ which takes values $\mu_1, \ldots, \mu_m$ with probabilities $p_1 \geq \ldots \geq p_m$. Then $c_{m-1}$ is constructed from a Huffman code $c_{m-1}$ for values $\mu_1, \ldots, \mu_{m-1}, \nu$ with probabilities $p_1, \ldots, p_{m-2}, p_{m-1}+p_m$.

Observe that $\E S_m = \E S_{m-1} + p_{m-1} + p_m$ by construction of $c_m$ from $c_{m-1}$.

Now let $c_m'$ be an optimal code for $X_m$. Without loss of generality, we may take $c_m'$ to be prefix-free and the last two codewords of $c_m'$ have maximal length and differ only in the last digit (see \textbf{3.2} below). Say $c_m'(\mu_{m-1}) = y0, c_m'(\mu_m) = y1$ for some $y \in \{0, 1\}^\ast$.

Let $c_{m-1}'$ be the prefix free code for $X_{m-1}$ given by $c_{m-1}'(\mu_i) = c_m'(\mu_i), c_{m-1}'(\nu) = y$.

Then the expected word length is $\E S_m' = \E S_{m-1}' + p_{m-1} + p_m \geq \E S_{m-1} + p_{m-1} + p_m = \E S_m$ by the inductive hypothesis, and so $c_m$ is optimal.
\end{proof}
\begin{lemma}
Suppose messages $\mu_1, \ldots, \mu_m$ are sent with probabilities $p_1,\ldots, p_m$, with an optimal code $c$ with word lengths $\ell_1, \ldots, \ell_m$. Then:
\begin{enumerate}
\item If $p_i > p_j$ then $\ell_i \leq \ell_j$.
\item Among all codewords of maximal length, there are two that differ only in the last digit.
\end{enumerate}
\end{lemma}
\begin{proof}
Otherwise, modify $c$ by swapping the $i\th$ and $j\th$ codewords, or deleting the last letter of each codeword of maximal length. The modified code is still prefix-free but has shorter expected word length, contradicting optimality of $c$.
\end{proof}
\section{Joint Entropy}
If $X,Y$ are random variables with value in $\mathscr{A}$ and $\mathscr{B}$. Then $(X,Y)$ is also a random variable with entropy $H(X,Y)$, the \emph{joint entropy} of $X,Y$.
\begin{align*}
H(X,Y) = -\sum_{x \in \mathscr{A}} \sum_{y \in \mathscr{B}} \P(X=x, Y=y)\log \P(X=x, Y=y)
\end{align*}
We can of course generalise this to any finite number of random variables. We will use Gibb's (\textbf{2.1}) to prove:
\begin{lemma}
Let $X,Y$ be random variables taking values in $\mathscr{A}, \mathscr{B}$. Then:\begin{align*}
H(X,Y) \leq H(X) + H(Y)
\end{align*}
with equality if and only if $X$ and $Y$ are independent.
\end{lemma}
\begin{proof}
Let $\mathscr{A} = \{x_1, \ldots, x_m\}, \mathscr{B} = \{y_1, \ldots, y_n\}$. Set $p_{ij} = \P(X=x_i, Y=y_i), p_i = \P(X=x_i), q_i = \P(Y=y_i)$. Then Gibb's inequality with $\{p_{ij}\}$ and $\{p_iq_j\}$ gives:
\begin{align*}
-\sum_{i,j} p_{ij}\log p_{ij} \leq -\sum_{i,j} p_{ij}\log(p_iq_j) &= -\sum_i\left(\sum_j p_{ij}\right) \log p_i - \sum_j \left(\sum_i p_{ij}\right) \log q_j\\
&= -\sum_i p_i \log p_i -\sum_j q_j \log q_j
\end{align*}
i.e. $H(X,Y) \leq H(X) + H(Y)$, with equality if and only if $p_{ij} = p_i q_j$ for all $i, j$, i.e. when $X,Y$ are independent.
\end{proof}

\hspace*{-1em}\underline{Example:} Let $X$ be a random variable that takes $D$ values with probability $\frac1D$. Then $H(X) = \log_2(D)$. Suppose $X_1, \ldots, X_N$ are i.i.d. with the same distribution as $X$. Then $H(X_1, \ldots, X_N) = N \log_2 D$.

\section{Error Correcting Codes}
\subsection{Noisy Channels and Hamming's Code}
A \emph{binary [n,m]-code} is a subset $C \subseteq \{0,1\}^n$ of \emph{size} $m = |C|$, \emph{length} $n$. The elements of $C$ are called \emph{codewords}. We use an $[n,m]$-code to send one of $m$ messages through a binary symmetric channel, making $n$ uses of the channel. Clearly $1 \leq m \leq 2^n$, so $0\leq \frac{1}{n}\log m \leq 1$. If $|C| = 1$ then $\rho(C) = 0$, and if $C= \{0,1\}^n$ then $\rho(C) = 1$.

For $x, y \in \{0,1\}^n$, the \emph{Hamming distance} $d(x,y) = |\{i:1\leq i\leq n, x_i \neq y_i \}|$, i.e. the number of positions where $x$ and $y$ differ.

We have three possible decoding rules:
\begin{enumerate}
\item The \emph{ideal observer} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ maximising $\P(c$ sent$|x$ received$)$.
\item Then \emph{maximum likelihood} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ maximising $\P(x$ received$|c$ sent$)$.
\item The \emph{minimum distance} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ minimising $d(x,c)$.
\end{enumerate}

\begin{lemma}
\item
\begin{enumerate}
\item If all the messages are equally likely, then 1. and 2. agree.
\item If $p < \frac{1}{2}$, then 2. and 3. agree.
\end{enumerate}
\end{lemma}
\begin{proof}
\item
\begin{enumerate}
\item By Bayes' Rule:
\begin{align*}
\P(c\sent |x \received) = \frac{\P(c \sent)}{\P(x\received)}\P(x\received|c\sent)
\end{align*}
\end{enumerate}
By Hypothesis, $\P(c\sent)$ is independent of $c \in C$, and so for fixed $x$, maximising $\P(c\sent|x\received)$ is the same as maximising $\P(x\received|c\sent)$.

\item Let $r = d(x,c)$. Then $\P(x\received|c\sent) = p^r(1-p)^{n-r} = (1-p)^n\left(\frac{p}{1-p}\right)^r$. Since $p < \frac12$, $\frac{p}{1-p} < 1$, and so maximising $\P(x\received|c\sent)$ is the same as minimising $d(x, c)$.
\end{proof}

For instance, suppose $000$ is sent with probability $\frac{9}{10}$, and $111$ with probability $\frac{1}{10}$, through a binary symmetric channel with error probability $\frac14$. If we receive $110$, the ideal receiver computes $\P(000\sent|110\received)=\frac34; \P(110\sent|110\received)=\frac14$, and so decodes it as $000$. But the minimum distance (and so maximal likelihood) code is $111$. Henceforth, we will decide to use minimal distance decoding. 

Note that minimal distance decoding can be expensive in terms of time and storage if $|C$ is large, and we also need to specify a convention in the case of a tie (e.g. make a random choice, request the message again).

A code is \emph{d-error detecting} if changing up to $d$ digits in each codeword can never produce another codeword. It is \emph{e-error correcting} if, knowing that $x \in \{0,1\}^n$ differs from some codeword in at most $e$ places, we can deduce uniquely what the codeword is.

\hspace*{-1em}\underline{Examples}
\begin{enumerate}
\item A \emph{repetition code} of length $n$ has codewords $00\ldots0, 11\ldots1$. This is an $[n,2]$-code. It is $(n-1)$ error detecting and $\floor{\frac{n-1}{2}}$-error correcting. But the information rate is only $\frac1n$.
\item A \emph{simple parity check code} or \emph{paper tape code}: identify $\{0,1\}$ with $\F_2$ (i.e. arithmetic modulo 2), and let $C = \{(x_1,\ldots, x_n)\in \{0,1\}^n : \sum x_i = 0\}$. This is an $[n, 2^{n-1}]$-code. It is 1-error detecting, but cannot correct errors. Its information rate is $\frac{n-1}{n}$.
\item \emph{Hamming's Original Code} is a $2$-error detecting and 1-error correcting binary [7,16]-code:
\begin{align*}
C = \left.\begin{cases} & c_1+c_3+c_5+c_7 = 0\\ c \in \F_2^7 : & c_2+c_3+c_6+c_7 = 0\\ &c_4+c_5+c_6+c_7 = 0\end{cases}\right\}
\end{align*}
\end{enumerate}
The bits $c_3,c_5,c_6,c_7$ are arbitrary and $c_1, c_2, c_4$ are forced. The information rate is $\frac47$.

Given $x \in \F_2^7$, we form the \emph{syndrome} $z = (z_1,z_2,z_4) \in \F_2^7$, where $z_1 = x_1+x_3+x_5+x_7,$ $z_2 = x_2+x_3+x_6+x_7,$ $z_4 = x_4+x_5+x_6+x_7$. If $x \in C$ then $z = (0,0,0)$. If $d(x,c) = 1$ for some $c \in C$ then $x_i$ and $c_i$ differ for $i = z_1 + 2z_2 + 4z_4$. This can be checked easily for $c=0$ with a case by case check of the seven binary sequences of six 0s and one 1, e.g. $x=0010000$ gives a syndrome $z = (1,1,0), i = 1+2+0 = 3$.

\begin{lemma}
$d$ is a metric on $\F_2^n$.
\end{lemma}
\begin{proof}
Immediately, $d(x,y) \geq 0$, with equality if and only if $x=y$, and $d(x,y)=d(y,x)$. For the triangle inequality, note that if $x$ and $z$ differ at position $i$ then either $x,y$ differ at $i$ or $y,z$ differ at $i$. So every difference appearing in $d(x,z)$ appears in $d(x,y)+d(y,z)$, so $d(x,z) \leq d(x,y)+d(y,z)$.
\end{proof}

Note that $d(x,y) = \sum_i d_1(x_i,y_i)$ where $d_1$ is the discrete metric on $\F_2$. We define the \emph{minimum distance} of a code to be $\min_{c_1 \neq c_2} d(c_1,c_2)$.

\begin{lemma}
Let $C$ have minimal distance $d$. Then:
\begin{enumerate}
\item $C$ is $(d-1)$-error detecting, but cannot detect all sets of $d$ errors.
\item $C$ is $\floor{\frac{d-1}{2}}$-error correcting, but cannot correct all sets of $\floor{\frac{d-1}{2}}+1$ errors.
\end{enumerate}
\end{lemma}
\begin{proof}
\item
\begin{enumerate}
\item $d(c_1,c_2) \geq d$ for all distinct $c_1, c_2 \in C$. So $C$ is $(d-1)$-error detecting. But $d(c_1, c_2)=d$ for some $c_1, c_2 \in C$. So $C$ cannot detect all sets of errors.

\item Define the closed Hamming ball with center $x \in \F_2^n$, radius $r\geq 0$ as $B(x,r) = \{y \in \F_2^n: d(x,y)\leq r\}$. Now $C$ is $e$-error correcting if and only if, for all $c_1 \neq c_2 \in C$, we have $B(c_1, e) \cap B(c_2, e) = \emptyset$.

If $x \in B(c_1,e) \cap B(c_2,e )$, then $d(c_1,c_2) \leq d(c_1,x)+d(x,c_2) \leq 2e$. So if $d \geq 2e+1$, then $C$ is $e$-error correcting, with $e = \floor{\frac{d-1}{2}}$. For the second part, take $c_1, c_2 \in C$ with $d(c_1, c_2) = d$. Then suppose $x \in \F_2^n$ differs from $c_1$ in $e$ digits where $c_1, c_2$ differ too. Then $d(x,c_1) = e, d(x,c_2) = d-e$. If $d < 2e$ then $B(c_1, e) \cap B(c_2, d-e) \neq 0$, and so $C$ cannot correct all sets of $e$-errors. Then take $e = \ceil{\frac{d}{2}} = \floor{d-1}{2}+1$.
\end{enumerate}
\end{proof}
As a point of a notation, an $[n,m]$-code with minimum distance $d$ will be denoted as an $[n,m,d]$-code.

\hspace*{-1em}\underline{Examples:}
\begin{enumerate}
\item Repetition of length $n$ is an $[n,2,n]$-code.
\item Simple parity check code of length $n$ is an $[n, 2^{n-1}, 2]$-code.
\item Hamming's code is 1-error correcting, so $d\geq 3$. 0000000, 1110000 are both codewords, so it is a $[7,16,3]$-code, and hence 2-error correcting.
\end{enumerate}

\section{Covering Estimates}
Denote $V(n,r) = |B(x,r)| = \sum_{i=0}^r \binom{n}{i}$, independent of $x \in \F_2^n$, as the \emph{volume} of the ball (i.e. the number of points it contains).

\begin{lemma}[Hamming's Bound]
An $e$-error correcting code of length $n$ has:
\begin{align*}
|C| \leq \frac{2^n}{V(n,e)}
\end{align*}
\end{lemma}
\begin{proof}
Suppose $C$ is $e$-error correcting. Then $B(c_1, e) \cap B(c_2, e) = \emptyset$ for all $c_1 \neq c_2 \in C$. Then $\sum_{c \in C} |B(c,e)| \leq |\F_2^n| = 2^n$, i.e. $|C|V(n,e) \leq 2^n$.
\end{proof}
A code $C$ of length $n$ that can correct $e$ errors is \emph{perfect} if $|C|=\frac{2^n}{V(n,e)}$. Equivalently, a code is perfect if for all $x \in \F_2^n$ there is a unique $c \in C$ such that $d(x,c) \leq e$, or $\F_2^n = \bigcup_{c\in C}B(c,e)$, i.e. any $e+1$ errors will make you decode incorrectly.

For example, Hamming's $[7,16,3]$-code is perfect, as $\frac{2^7}{7+1} = 2^4 = |C|$. Note that if $\frac{2^n}{V(n,e)} \notin \Z$ then there is no perfect $e$-error correcting code of length $n$, and even if $2^n/V(n,e)$ is an integer is may be the case that no perfect code exists.

Define \emph{A(n,d)} $= \max\{m : \exists [n,m,d]$-code$\}$. For instance, $A(n,1) = 2^n, A(n,n) = 2, A(n,2) = 2^{n-1}$.

\begin{lemma}
$A(n,d+1) \leq A(n,d)$
\end{lemma}
\begin{proof}
Let $m = A(n,d+1)$, and pick a code $C$ with parameters $[n,m,d+1]$. Let $c_1, c_2 \in C$ with $d(c_1,c_2) = d+1$. Let $c_1'$ differ from $c_1$ in exactly one of the places where $c_1, c_2$ differ. Then $d(c_1', c_2) = d$. If $c \in C\setminus\{c_1\}$, then $d(c,c_1)\leq d(c,c_1')+d(c_1', c_1) \implies d(c_1, c_1') \geq d$.

Replacing $c_1$ by $c_1'$ gives an $[n,m,d]$-code i.e. $m \leq A(n,d)$.
\end{proof}

\begin{corollary}
Equivalently, $A(n,d) = \max\{m : \exists [n,m,d']-$code for some $d'\geq d\}$.
\end{corollary}
\begin{theorem}
\begin{align*}
\frac{2^n}{V(n,d-1)} \leq A(n,d) \leq \frac{2^n}{V(n, \floor{\frac{d-1}{2}})}
\end{align*}
The lower bound is called the Gilbert-Shannon-Varshanov (GSV) bound, whilst the upper bound follows from Hamming's bound.
\end{theorem}
\begin{proof}[Proof of GSV]
Let $m = A(n,d)$, and let $C$ be a $[n,m,d]$-code. Then there cannot exist $x \in \F_2^n$ with $d(x,c) \geq d$ for all $c \in C$, otherwise we could replace $C$ by $C \cup \{x\}$, contradicting maximality of $d(x,c)$. Hence $\F_2^n = \bigcup_{c\in C} B(c, d-1)$. Hence $2^n \leq m V(n, d-1)$.
\end{proof}
For example, take $n=10, d=3$. Then $V(n,2) = 56, V(n,1)=11$, and so these bounds give $\frac{2^10}{56} \leq A(10,3) \leq \frac{2^10}{11}$, i.e. $19\leq A(10,3)\leq 93$, but in fact we know computationally that it is between 72 and 79.

\subsection{Asymptotics}
We study $\frac{\log A(n, \floor{n\delta})}{n}$ as $n \to \infty$ to see how large the information rate can be for a given error rate.
\begin{proposition}
Let $0 < \delta < \frac12$. Then:
\begin{enumerate}
\item $\log V(n, \floor{n \delta}) \leq n H(\delta)$.
\item $\frac1n \log A(n, \floor{n\delta}) \geq 1- H(\delta)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Assuming \textit{1.} we see by the GSV bound, $A(n, \floor{n\delta}) \geq \frac{2}{V(n, \floor{n\delta}-1} \geq \frac{2^n}{V(n, \floor{n\delta})}$, so $\frac{\log A(n, \floor{n\delta})}{n} \geq 1 - \frac{\log V(n, \floor{n\delta})}{n} \geq 1- H(\delta)$, and so \textit{1.}$\implies$\textit{2.}

For \textit{1.} observe $H(\delta)$ is increasing for $\delta \leq \frac12$, so \textsc{wlog} we can assume $n\delta \in \Z$. Then:
\begin{align*}
1 &= (\delta + (1-\delta))^n\\
&= \sum_{i=0}^n \binom{n}{i} \delta^i (1-\delta)^{n-i}\\
&\geq \sum_{i=0}^{n\delta} \binom{n}{i} \delta^i (1-\delta)^{n-i}\\
&= (1-\delta)^n \sum_{i=0}^{n\delta} \binom{n}{i} \left(\frac{\delta}{1-\delta}\right)^i\\
&\geq (1-\delta)^n \sum \binom{n}{i} \left(\frac{\delta}{1-\delta}\right)^{n\delta}\\
&= \delta^{n\delta}(1-\delta)^{n(1-\delta)}V(n,n\delta)\\
0 &\geq n\delta\log\delta + n(1-\delta)\log(1-\delta)+\log V(n,n\delta)\\
0 &\geq -nH(\delta) + \log V(n,n\delta)
\end{align*}
\end{proof}
This constant $H(\delta)$ is the best possible, in the sense that:
\begin{proposition}
\begin{align*}
\lim_{n\to\infty} \frac{\log V(n, \floor{n\delta})}{n} = H(\delta)
\end{align*}
\end{proposition}
\begin{proof}
\textsc{Wlog} we may assume that $0 < \delta < \frac12$. Let $0 \leq r \leq \frac{n}{2}$. Recall that $V(n,r) = \sum_{i=0}^r \binom{n}{i}$. Then:
\begin{align*}
\binom{n}{r} \leq V(n,r) \leq (r+1)\binom{n}{r} \tag{(\ast)}
\end{align*}
From Stirling's approximation, $\log\binom{n}{r} = -r\log \frac{r}{n} - (n-r)\log\frac{n-r}{n} + \mathcal{O}(\log n) = nH(\frac{r}{n}) + \mathcal{O}(\log n)$.

Then from $(\ast)$, we have:
\begin{align*}
H(\frac{r}{n}) + \mathcal{O}\left(\frac{\log n}{r}\right) \leq \frac{\log V(n,r)}{n} \leq H\left(\frac{r}{n}\right) + \mathcal{O}\left(\frac{\log n}{n}\right)\\
\therefore \lim_{n\to\infty} \frac{\log V(n, \floor{n\delta})}{n} = H(\delta)
\end{align*}
\end{proof}
\section{New Codes from Old}
Suppose $C$ is an $[n,m,d]$-code. Then the \emph{parity check digit extension} $C^+$ of $C$ is the code of length $n+1$ given by:
\begin{align*}
C^+ = \{(c_1, \ldots, c_n, \sum_{i=1}^n c_i) : (c_1, \ldots, c_n) \in C\}
\end{align*}
where the summation is done modulo 2. It is an $[n+1, m, d']$-code where $d' = d$ or $d+1$.

We can also delete the $i\th$ digit from each codeword for $1 \leq i\leq n$, giving a \emph{truncated} or \emph{punctured} codeword (depending on if $i=n$ or $i<n$ respectively), called $C^-$, with parameters $[n-1, m, d']$ where $d-1 \leq d' \leq d$.

Finally, given some $1 \leq i \leq n, \alpha \in \F_2$, we can create the \emph{shortened} or \emph{punctured} code $C'$ of $C$ is $\{(c_1, \ldots, c_{i-1},c_{i+1}, \ldots, c_n) : (c_1, \ldots, c_{i-1}, \alpha, c_{i+1} \ldots, c_n) \in C\}$. This has parameters $[n-1, m', d']$ with $d' \geq d$ and $m' \geq \frac{m}{2}$ for $c$ a suitable choice of $\alpha$.

\section{AEP and Shannon's First Coding Theorem}
A \emph{source} is a sequence of random variables $X_1, X_2, \ldots$ taking values in some alphabet $\mathscr{A}$. A source is \emph{Bernoulli} or \emph{memoryless} if $X_1, \ldots$ are independently identically distributed (\textsc{iid}). A source $X_1, \ldots$ is \emph{reliably encodable at rate r} if there are subsets $A_n \subseteq \mathscr{A}^n$ such that:
\begin{enumerate}
\item $\lim_{n\to \infty} \frac{\log|A_n|}{n} = r$
\item $\lim_{n\to \infty} \P[(X_1, \ldots, X_n)\in A_n] = 1$
\end{enumerate}
The \emph{information rate} $H$ of a source is the infimum of all reliable encoding ratesso that $0 \leq H \leq \log |\mathscr{A}|$. Shannon's first coding theorem computes the information rate of certain sources, including Bernoulli sources.

\subsection{Reminder from 1A Probability}
A \emph{probability space} is given by a triple $(\Omega, \mathscr{F}, \P)$ where $\mathscr{F} \subset \powset(\Omega)$ is a set of \emph{events} and $\P$ is a \emph{probability measure}, and a \emph{random variable} $X$ is a function defined on $\Omega$ with some range. It has a \emph{probability mass function} $p: x \mapsto \P(X=x)$

We say that a sequence of random variables $X_1, X_2, \ldots$ \emph{converges in probability} to $\lambda \in \R$ means that
\begin{align*}
\forall\;\epsilon > 0, \lim_{n \to \infty} \P(|X_n -\lambda| \geq \epsilon) = 0
\end{align*}
We write $X_n \xrightarrow{\P} \lambda$ as $n \to \infty$.

\begin{theorem}[Weak Law Of Large Numbers, \textsc{wlln}]
Let $X_1, X_2, \ldots$ be \textsc{iid} discrete real-valued random variables with finite expected value $\mu$. Then:
\begin{align*}
\frac1n \sum_{i=1}^n X_i \xrightarrow{\P} \mu\text{ as } n\to \infty
\end{align*}
\end{theorem}
\begin{proof}
See Carne, theorem 10.3.
\end{proof}
\begin{lemma}
The information rate of a Bernoulli source $X_1, X_2, \ldots$ is at most the expected word length of an optimal code $c:\mathscr{A} \to \{0,1\}^{\ast}$ for $X_i$.
\end{lemma}
\begin{proof}
Let $\ell_1, \ell_2, \ldots$ be the lengths of codewords when we encode $X_1, X_2, \ldots$ using $c$. Then given $\epsilon >0$, let $A_n = \{x \in \mathscr{A}^n : c^{\ast}(x)$ has length $< n(\E[\ell_i] + \epsilon)\}$. Then:
\begin{align*}
\P\left[(X_1, \ldots, X_n) \in A_n\right] &= \P\left[\sum \ell_i < n(\E[\ell_i] +\epsilon)\right]\\
&\geq \P\left(\left|\frac1n \sum\ell_i - \E[\ell_i]\right| < \epsilon\right)\\
&\to 1 \text{ as } n \to \infty
\end{align*}
Now $c$ is decipherable so $c^{\ast}$ is injective, and hence $|A_n| \leq 2^{n(\E[\ell_i]+\epsilon)}$. Making $A_n$ larger if required, we may take $|A_n| =\floor{2^{n(\E[\ell_i]+\epsilon)}}$. Hence $\frac{\log|A_n|}{n} \to \E[\ell_i] + \epsilon$. So $X_1, X_2, \ldots$ is reliably encodable at a rate $r = \E[\ell_i] +\epsilon$ for any $\epsilon > 0$, and hence the information rate is at most $\E[\ell_i]$.
\end{proof}
\begin{corollary}
A Bernoulli source has information rate less than $H(X_1) + 1$.
\end{corollary}
\begin{proof}
Use \textbf{8.2} and the Noiseless Coding theorem \textbf{2.3}.
\end{proof}

Now suppose we encode $X_1, X_2, \ldots$ in blocks:
\begin{align*}
\underbrace{X_1, \ldots, X_N}_{Y_1}, \underbrace{X_{N+1}, \ldots, X_{2N}}_{Y_2}, \ldots
\end{align*}
such that $Y_1, Y_2, \ldots$ take values in $\mathscr{A}^N$. We can check that if $X_1,  X_2, \ldots$ has information rate $H$, then $Y_1, Y_2, \ldots$ has information rate $NH$.

\begin{proposition}
The information rate$H$ of a Bernoulli source $X_1, X_2, \ldots$ is at most $H(X_1)$.
\end{proposition}
\begin{proof}
Apply \textbf{8.3} to $Y_1, Y_2, \ldots$ to get:
\begin{align*}
NH < H(Y_1) + 1 = H(X_1, \ldots, X_N) + 1 = \sum_{i=1}^N H(X_i) + 1 = NH(X_i)+1
\end{align*}
i.e. $H < H(X_1) + \frac1N$ for all $N \geq 1$, and so $H \leq H(X_1)$.
\end{proof}

\hrule
\subsection{Typical Sequences}
This content is nonexaminable, but is required to prove the examinable result that $H = H(X_1)$.

\hspace*{-1em}As a motivational example, toss a biased coin with head probability $p$, and let $X_i$ be the outcome of the $i\th$ flip. If we toss a large number, say $N$, times, we expect that we will get about $pN$ heads and $(1-p)N$ tails. The probability of any particular sequence of $pN$ heads and $(1-p)N$ tails is $p^{pN}(1-p)^{(1-p)N} = 2^{-NH(X)}$.

We say that a source $X_1, X_2, \ldots$ satisfies the \emph{Asymptotic Equipartition Property (AEP)} for some constant $H \geq 0$ if:
\begin{align*}
-\frac1n\log p(X_1, \ldots, X_n) \to H \text{ as } n \to \infty
\end{align*}
\begin{lemma}
The AEP for a source $X_1, x_2, \ldots$ is equivalent to the following:\\
$\forall\;\epsilon>0 \exists n_0(\epsilon) \st \forall\;n\geq n_0(\epsilon) \exists T_n \subseteq \mathscr{A}^n \st$
\begin{itemize}
\item $P[(X_1, \ldots, X_n) \in T_n] > 1-\epsilon$
\item $\forall\;(x_1, \ldots, x_n) \in T_n, 2^{-n(H+\epsilon)} \leq p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)}$
\end{itemize}
The $T_n$ are called \emph{typical sets}, and the $(x_1, \ldots, x_n) \in T_n$ are \emph{typical sequences}.
\end{lemma}
\begin{proof}
If $(x_1, \ldots, x_n) \in \mathscr{A}^n$ then we have the following equivalence:
\begin{align*}
2^{-n(H+\epsilon)} \leq p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)} \iff |-\frac1n \log p(x_1, \ldots, x_n) - H| \leq \epsilon \tag{\dagger}
\end{align*}
Both AEP and the claimed equivalent results say that $P((X_1, \ldots, X_N)\text{ satisfies } \dagger) \to 1$ as $n \to \infty$.
\end{proof}

\begin{theorem}[Shannon's First Coding Theorem]
If a source $X_1, X_2, \ldots$ satisfies the AEP with constant $H$ then it has information rate $H$.
\end{theorem}
\begin{proof}
Let $\epsilon >0$ and let $T_n \subseteq \mathscr{A}^n$ be typical sets. Then for all $(x_1, \ldots, x_n) \in T_n$:
\begin{align*}
p(x_1, \ldots, x_n) \geq 2^{-n(H+\epsilon)} \implies 1 \geq |T_n|2^{-n(H+\epsilon)} \implies \frac{\log|T_n|}{n} \leq (H+\epsilon)
\end{align*}
Taking $A_n = T_n$ in the definition of reliable encoding, we see that the source is reliably encodeable at rate $H+\epsilon$. As $\epsilon >0$, the information rate is $\leq H$.

Conversely, if $H=0$ we're done, otherwise pick $0 <\epsilon < \frac{H}{2}$. Suppose for a contradiction that the source is reliably encodeable at rate $H-2\epsilon$, say, with sets $A_n \subseteq \mathscr{A}^n$. Let $T_n \subseteq \mathscr{A}^n$ be typical sets. Then for all $(x_1, \ldots, x_n) \in T_n, p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)}$

Hence $\P(A_n \cap T_n) \leq 2^{-n(H-\epsilon)}$, and so $\frac{\log\P(A_n\cap T_n)}{n} \leq (H-\epsilon) + \frac{\log |A_n|}{n}\xrightarrow{n \to \infty} -\epsilon$. So $\P(A_n \cap T_n) \to 0$ as $n \to \infty$. But $\P(T_n) \leq \P(T_n\cap A_n) + \P(\mathscr{A}^n \setminus A_n) \to 0 + 0 = 0$ as $n \to \infty$, contradicting typicality of $T_n$. Hence the source cannot be reliably encoded at rate $H-2\epsilon$, and so the information rate must be $\geq H$, and hence $=H$.
\end{proof}
\section{Capacity and Shannon's Second Coding Theorem}
Given a random variable $X$ with mass function $p_X$, we can construct a new random variable $p(X) = p_X \circ X$, taking values in $[0,1]$. Then $H(X) = \E(-\log p(X))$. For example, if $X,Y$ are independent, then $p(X,Y) = p(X)p(Y)$, and so $-\log p(X,Y) = -\log p(X) - \log p(Y) \implies H(X,Y) = H(X)+H(Y)$.

\begin{corollary}
A Bernoulli source $X_1, X_2, \ldots$ has information rate $H(X_1) = H$.
\end{corollary}
\begin{proof}[Proof*]
$p(X_1, \ldots, X_n) = p(X_1)\ldots p(X_n)$, and hence we have:
\begin{align*}
-\frac{\log p(X_1, \ldots X_n)}{n} = -\frac{1}{n}\sum_{i=1}^n \log p(X_i) \xrightarrow{\P} H(X_i)
\end{align*}
by the weak law of large numbers, and using the fact that the $X_i$ are i.i.d. Check as an exercise that the AE holds with constant $H(X_1)$ using the definition of convergence in probability. Hence by \textbf{8.6} we are done.
\end{proof}
Note that \textbf{8.4} gave us an information rate $\leq H(X_1)$, without the use of the AEP. The AEP can also be used for noiseless coding - we encode the typical sequences with a block code and the atypical sequences arbitrarily, since they rarely occur. Many sources of interest, not just Bernoulli sources, satisfy the AEP. Under suitable conditions, the sequence $\frac1n H(X_1, \ldots, X_n)$ is decreasing and the AEP is satisfied with constant $H \lim_{n\to \infty} \frac{H(X_1,\ldots, X_n}{n}$.

Consider a communication channel with input of alphabet $\mathscr{A}$, and output $B$. A code of length $n$ is a subset $C \subseteq \mathscr{A}^n$. The \emph{error rate}
\begin{align*}
\hat{e}(C) = \max \P(\text{error }|c\text{ sent})
\end{align*}

The \emph{information rate} is $\rho(C) = \frac{\log |C|}{n}$.

The channel can \emph{transmit reliably} at rate $R$ if there are codes $C_1, C_2, \ldots$ with $C_n$ of length $n$, and:
\begin{itemize}
\item $\lim_{n \to \infty} \rho(C_n) = R$.
\item $\lim_{n \to \infty} \hat{e}(C_n) = 0$.
\end{itemize}
The \emph{operational capacity} is the supremum of all reliable transmission rates.

Assume a source has information rate $r$ bits per symbol, and emits symbols at $s$ symbols per second, whilst the channel has capacity $R$ bits per transmission and can can transmit symbols at $S$ transmissions per second. Usually $S=s=1$. If $rs \leq RS$ then we can encode and transmit reliably, and if $rs > RS$ we cannot.

\begin{proposition}
A binary symmetric channel with error probability $p < \frac{1}{4}$ has non-zero capacity.
\end{proposition}
\begin{proof}
We use the $GSV$ bound. Pick $\delta$ with $2p <\delta  < \frac12$. We claim reliable transmission rate of $R = 1-H(\delta) > 0$.

Let $C_n$ be a code of length $n$ with minimum distance $\floor{n\delta}$ of maximal size. Them $|C_n| = A(n, \floor{n\delta}) \geq 2^{n(1-H(\delta)} = 2^{nR}$.

Replacing $C_n$ by a subcode we can assume $|C_n| = \floor{2^{nR}}$ with minimum distance still $\geq \floor{n\delta}$

Now, with minimum distance decoding, $\hat{e}(C_n) \leq \P($in $n$ uses the BSC makes more than $\frac{n\delta-1}{2}$ errors$)$.

Pick $\epsilon > 0$ with $p+\epsilon < \frac{\delta}{2}$. For $n$ sufficiently large we have that $\frac{n\delta-1}{2} = n(\frac{\delta}{2} - \frac{1}{2n}) > n(p+\epsilon)$.

Hence $\hat{e}(C) \leq \P($BSC makes more than $n(p+\epsilon)$ errors$) \to 0$, as we will see in the next lemma.
\end{proof}
\begin{lemma}
Let $\epsilon > 0$. A BSC with error probability $p$ is used to transmit $n$ digits. Then:
\begin{align*}
\lim_{n \to \infty} \P(\text{BSC makes at least }n(p+\epsilon)\text{ errors}) = 0
\end{align*}
\end{lemma}
\begin{proof}
If $U_i$ is the Bernoulli random variable that digit $i$ is mistransmitted. Then $U_i$ are i.i.d. with probability $p$. So $\E[U_i] = p$. Then the probability we are interested in is $\P(\sum U_i \geq n(p+\epsilon)) \leq \P(|\frac{1}{n}\sum U_i -p| \geq \epsilon) \to 0$ by the \textsc{wlln}.
\end{proof}

\subsection{Fano's Inequality}
Let $X,Y$ be random variables taking values in the alphabets $\mathscr{A}, \mathscr{B}$. Then:
\begin{itemize}
\item $H(X|Y=y) = -\sum_{x \in \mathscr{A}} \P(X=x|Y=y)\log \P(X=x|Y=y)$
\item $H(X|Y) = \sum_{y \in \mathscr{B}} \P(Y=y)H(X|Y=y)$
\end{itemize}
Clearly $H(X|Y) \geq 0$.
\begin{lemma}
\begin{align*}
H(X,Y) = H(X|Y) + H(Y)
\end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
H(X|Y) &= -\sum_{x\in\mathscr{A}} \sum_{y \in \mathscr{B}} \P(X=x|Y=y)\P(Y=y)\log \P(X-x|Y=y)\\
&= -\sum_{x\in\mathscr{A}} \P(X=x, Y=y) \log \frac{\P(X=x, Y=y}{\P(Y=y)}\\
&= -\sum_{(x,y)\in\mathscr{A}\times\mathscr{B}}\underbrace{\P(X=x,Y=y)\log\P(X=x,Y=y)}_{H(X,Y)} + \underbrace{\P(X=x, Y=y)\log\P(Y=y)}_{-H(Y)}\\
&= H(X,Y) - H(Y)
\end{align*}
\end{proof}
\begin{corollary}
$H(X|Y) \leq H(X)$ with equality if and only if $X,Y$ are independent.
\end{corollary}
\begin{proof}
Combine \textbf{4.1} with \textbf{9.4}.
\end{proof}
We can replace $X,Y$ by random variables $X_1, X_2, \ldots, X_r$ and $Y_1, Y_2, \ldots, Y_s$, and similarly define $H(X_1, \ldots, X_r|Y_1, \ldots, Y_s)$\footnote{$H(X,Y|Z)$ means ``the entropy of ($X$ and $Y$) given $Z$", \textsc{not} ``the entropy of $X$ and ($Y$ given $Z$)"}.

\begin{lemma}
\begin{align*}
H(X|Y) \leq H(X|Y, Z) + H(Z)
\end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
H(X,Y,Z) &= H(Z|X,Y) + H(X,Y) = H(Z | X, Y) + H(X|Y) + H(Y)\\
H(X,Y,Z) &= H(X|Y,Z) + H(Y,Z) = H(X|Y, Z) + H(Z|Y) + H(Y)\\
\therefore H(X|Y) &= -H(Z|X,Y) + H(X|Y,Z) + H(Z|Y)\\
&\leq H(X|Y,Z) + H(Z) 
\end{align*}
\end{proof}
\begin{proposition}[Fano's Inequality]
Let $X,Y$ be random variables taking values in $\mathscr{A}$, where $|\mathscr{A}| = m$. Let $p = \P(X\neq Y)$. Then:
\begin{align*}
H(X|Y) \leq H(p) + p \log(m-1)
\end{align*}
\end{proposition}
\begin{proof}
Let $Z = \begin{cases} 0 & X = Y \\ 1 & X \neq Y\end{cases}$. Then $\P(Z=0) = 1-p, \P(Z=1) = p$. So $H(Z) = H(p)$. By \textbf{9.6},
\begin{align*}
H(X|Y) \leq H(p) + H(X|Y,Z)\tag{\ast}
\end{align*}
Then we have two cases:
\begin{itemize}
\item[$Z=0$:] We must have $X=y$, so $H(X|Y=y, Z=0) = 0$.
\item[$Z=1$:] Just $m-1$ remaining possibilities for $X$, so $H(X|Y=y, Z=1) \leq \log(m-1)$.
\end{itemize}
Hence we have:
\begin{align*}
H(X|Y,Z) &= \sum_{y,z} \P(Y=y, Z=z)H(X|Y=y,Z=z)\\
&\leq \sum_y \P(Y=y, Z=1)\log(m-1)\\
&= \P(Z=1)\log(m-1) = p\log(m-1)
\end{align*}
Then by $(\ast)$, $H(X|Y) \leq H(p) + p \log(m-1)$.
\end{proof}
We will apply this result later when $X$ takes values in $\mathscr{A}$ and $Y$ is the result of passing codewords through a channel and then decoding, where $p$ will be the probability of error.

If $X,Y$ are random variables, then we define the \emph{mutual information} of $X$ and $Y$ to be:
\begin{align*}
I(X;Y) \coloneqq H(X) - H(X|Y)
\end{align*}
By \textbf{4.1} and \textbf{9.4}, $I(X;Y) = H(X) + H(Y) - H(X,Y) \geq 0$, with equality if and only if $X$ and $Y$ are independent. We also see from this form that $I(X;Y) = I(Y;X)$.

Given a DMC with input alphabet $\mathscr{A}$ of size $m$, and output alphabet $\mathscr{B}$. Let $X$ be a random variable taking values in $\mathscr{A}$ used as an input to the channel. Let $Y$ be the random variable output, depending on both $X$ and the channel matrix.

We define the \emph{information capacity} to be the $\max_X I(X;Y)$, where the maximum is taken over all probability distributions $(p_1, \ldots, p_m)$ for $X$. Since the space of random variables for $X$ is a closed and bounded subset of $\R^m$, by Heine-Borel it is compact and, since $I$ is continuous, the maximum is attained. Note that the information capacity depends only on the channel matrix.

\begin{theorem}[Shannon's Second Coding Theorem]
For a DMC, the operational capacity is equal to the information capacity.
\end{theorem}
We will prove $\leq$ in general, and $\geq$ for a binary symmetric channel only. 

For example, with a BSC with error probability $p$, input $X$, output $Y$, then $\P(X=0) = \alpha, \P(X=1) = 1-\alpha$, and so $\P(Y=0) = \alpha(1-p) + (1-\alpha)p; \P(Y=1) = (1-\alpha)(1-p) + \alpha p$.

Then $C = \max_{\alpha} I(X;Y) = \max_\alpha \left[H(\alpha(1-p) + (1-\alpha)p) - H(p)\right] = 1-H(p$, where the max attained at $\alpha = \frac12$. So the information capacity $C = 1+p\log p + (1-p)\log(1-p)$. We can plot this on a graph:
\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[ymin=0, ymax=1, xmin=0, xmax=1]
\addplot[samples=200, blue, ultra thick] (x, {1+x*log2(x) + (1-x)*log2(1-x)});
\end{axis}
\end{tikzpicture}
\end{figure}
At $p=0,1$ the channel transmits perfectly, whilst at $p=\frac{1}{2}$ no information can be transmitted. We can choose to calculate $H(Y)-H(Y|X)$ or $H(X)-H(X|Y)$ to find the information - often one is easier than the other, for example with the binary erasure channel with erasure probability $p$.

$\P(X=0) = \alpha, \P(X=1) = 1-\alpha, \P(Y = 0) = \alpha(1-p), \P(Y=\ast) = p, \P(Y=1) =(1-\alpha)(1-p)$

Then $H(X|Y=0) = 0, H(X|Y = \ast) = H(\alpha), H(X|Y = 1) = 0$, and so $H(X|Y) = pH(\alpha)$.
\end{document}