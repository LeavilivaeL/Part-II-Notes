\documentclass[10pt,a4paper]{article}
\input{./preamble.tex}

\title{Ew Applied Maths}
\begin{document}
\maketitle
\setcounter{section}{-1}

\section{Introduction}
\subsection*{What is information?}
We get information when we acquire knowledge. In classical information, the fundamental unit of information is a bit - a \underline{B}oolean un\underline{it}, which takes values of either $0$ or $1$. If we have a question with $2^n$ possible different, answers, then we can label each answer with a string of $n$ bits.

\subsection*{What is computation?}
Computation can be defined as the processing of information - updating a bit string using a prescribed sequence of steps, which we might call a program. Each of these steps can be the actions of a Boolean gate, such as \textsc{Not}, \textsc{And}, \textsc{Or}.

\subsection*{What is a bit physically?}
Physically a bit is given by any 2 \textit{different} physical states of a system that can be reliably distinguished by some physical measurement. For instance, I can write 0 or 1 in this document, which is stored as data on GitHub or your hard drive, and you can conduct the physical measurement of loading this document and reading the following number: 1. This concept is summed up in the saying ``There is no information without representation". We see that any computation is then the manipulation of a physical system, and so computers must obey the laws of physics. Modern computers obey classical physics (at least for now - please email me / hololens call me if this is not the case for you), but as Feynmann once said: ``Nature isn't classical dammit".

On the atomic and subatomic (and even now molecular) scales, particles are governed by Quantum Mechanics. This has some novel features over classical mechanics, such as the ideas of quantum superposition, entanglement, and quantum measurement. Throughout this course, we will be exploiting these features. They will make things more complicated, but at the same time will give us some advantages in information storage, communication, computation, as well as security/cryptography.

Most of the work for this course was done during the 1980's and 1990's, although the Russian mathematician Alexander Holevo was already working on the ideas of quantum information in the 1970's.

\subsection*{In what ways is QIC better than its classical counterpart?}
A quantum computer cannot compute any computational task which isn't already computable \textit{in principle} by a classical computer. However, some things will require many times the age of the universe to compute even on the fastest of classical supercomputers.

\subsection{Computational Complexity Theory}
We want to have some idea of the ``difficulty" of a computational task. We measure this by the amount required of the two computational resources of time and space. For instance, consider the problem of factoring a number with $n$ digits. Then the input size for the computation is $n$, and we can look at how the time/space grow as functions of $n$.

If the time grows polynomially in $n$, we call this a poly-time algorithm. These are computable in practice. If it grows faster than any polynomial, we call it an exponential-time algorithm, which are, whilst theoretically computable, uncomputable in practice for large inputs. 

Currently, there is no known poly-time factoring algorithm for a classical computer, but in 1994 Peter Shor came up with a poly-time quantum algorithm for integer factorisation.

\subsection{Communication \& Security Issues}
If we use quantum particles as information carriers, we end up with communication possibilities which were the stuff of science fiction previously, such as quantum teleportation, or implement mathematically provably secure communication. 

\subsection{Technological Issue}
Moore's law states that, since 1965, devices will miniaturise by a factor of 4 every 3.5 years. Eventually (and this is already happening), we will get close to quantum scales. For instance, modern CPUs are built by manufacturing machines with a resolution of around 10nm, or 100 atoms across. At these scales, quantum mechanics will start to cause problems for classical computation, so why not embrace this new feature? In reality, the technological barrier to actually using quantum bits (qubits) is huge, and the most advanced current quantum computers have about 53 qubits. Moreover, Google has claimed \emph{quantum supremacy} in demonstrating a task, known as the ``random sampling algorithm", which their quantum computer can complete faster than a classical computer.

\subsection{Quantum Mechanics}
We have 4 key postulates of quantum mechanics:
\begin{itemize}
\item[(QM1)] Describes the quantum state of a physical system, $S$
\item[(QM2)] Describes the joint state of two composite systems, $S_1S_2$
\item[(QM3)] Describes how quantum states evolve throughout time. Here we will be interested in discrete time steps.
\item[(QM4)] Describes how quantum states respond to measurement.
\end{itemize}
The idea behind the mathematics of quantum mechanics, which we'll get to properly in the next section, is that we want to associate to every quantum-mechanical system $S$ a \emph{complex inner product vector space} $\mathcal{V}$, or \emph{Hilbert space}. We will be using \emph{Dirac's bra-ket notation}.

\section{Mathematical Preliminaries \& Bra-Ket Notation}
We will be working in an $n$-dimensional complex inner product spaces $\mathcal{V}$. We will denote a column vector by a \emph{ket}, denoted $\ket{\psi}$. It's conjugate transpose is called a \emph{bra}, and written $\bra{\psi}$. For instance, if $\ket{\psi} = \begin{pmatrix} a \\ b \end{pmatrix}$, then $\bra{\psi} = (\bar{a}, \bar{b})$. We then denote the \emph{inner product} $(\ket{\phi}, \ket{\psi}) = \braket{\phi}{\psi} = (\bar{a}, \bar{b})\begin{pmatrix} c \\ d\end{pmatrix} = \bar{a}c+\bar{b}d \in \C$.

Recall the axioms for an inner product:
\begin{itemize}
\item[IP1] Positive definite. $\R \ni \braket{\psi}{\psi} \geq 0$, and $=0$ if and only if $\ket{\psi} = \mathbf{0}$.
\item[IP2] Linearity in second argument. $\braket{\phi}{\lambda\psi} = \lambda \braket{\phi}{\psi}$
\item[IP3] Antilinearity in first argument. $\braket{\lambda\phi}{\psi} = \bar{\lambda}\braket{\phi}{\psi}$
\item[IP4] Skew-symmetry. $\overline{\braket{\phi}{\psi}} = \braket{\psi}{\phi}$
\item[IP5] Norm. We define $||\psi|| = \sqrt{\braket{\psi}{\psi}}$
\end{itemize}

For this course we will take as a convention that $\mathcal{V} = \C^n$, with basis $\ket{0} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \ket{1} = \begin{pmatrix}
0 \\ 1 \end{pmatrix}$, and so on for higher dimensions.

We also have an \emph{outer product}, which is simply the other way round, so that we have $\ket{\psi}\bra{\phi}$ is an $n\times n$ matrix.

Note that this basis is a \emph{complete orthonormal basis} for $\C^n$, i.e. we have $\braket{i}{j} = \delta_{ij}$.

By complete, we mean that $\sum_{i=1}^n \ket{i}\bra{j} = I_{n \times n}$. As an exercise, check that $\mathcal{V} = \C^2$ with basis $\ket{0}, \ket{1}$ is complete.

We will also be using the orthonormal basis $\{\ket{+}, \ket{-}\}$, with $\ket{\pm} = \frac{\ket{0}\pm\ket{1}}{\sqrt{2}}$.

Suppose we have spaces $\mathcal{V}, \mathcal{W}$, of dimensions $n, m$, with bases $\{\ket{e_i}\}_{i=1}^m; \{\ket{f_i}\}_{i=1}^n$. Then we define the \emph{tensor product space} $\mathcal{V} \otimes \mathcal{W}$ to be the $mn$ dimensional vector space with orthonormal basis $\{\ket{e_i}\otimes\ket{f_j}\}_{i,j}$, where we have:
\begin{align*}
\ket{v}\otimes\ket{w} = \begin{pmatrix} a \\ b \end{pmatrix} \otimes \begin{pmatrix} c \\ d \end{pmatrix} = \colvec{a \colvec{c \\ d}\\b \colvec{c\\d}} = \colvec{ac \\ ad\\ bc \\ bd} \in \C^2\otimes \C^2.
\end{align*}

If a vector $\ket{\psi} \in \mathcal{V}\otimes\mathcal{W}$ can be written as $\ket{v}\otimes \ket{w}$, we call it a \emph{product state}. There are some vectors which cannot be expressed in this way. These are called \emph{entangled vectors}. Note that we will often omit the $\otimes$, and just write $\ket{\psi} = \ket{v}\ket{w}$.

We can also take $k$-fold \emph{tensor powers} of $\mathcal{V}$:
\begin{align*}
\mathcal{V}^{\otimes k} \coloneqq \mathcal{V}\otimes \mathcal{V}\otimes\ldots \otimes \mathcal{V}
\end{align*}
$\mathcal{V}^{\otimes k}$ is an $n^k$ dimensional vector space. It has a basis $\{\ket{i_1}\ket{i_2}\ldots\ket{i_k} = \ket{i_1i_2\ldots i_k}\}$, where the $\ket{i_j}$ are basis vectors of $\mathcal{V}$. In the case where $\mathcal{V} = \C^2$, we can think of these as bitstrings.

For an example of an entangled vector, consider $\ket{\Phi} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11}) \in \C^2 \otimes \C^2$. Assume there is $\ket{v} = a\ket{0} + b\ket{1}; \ket{w} = c\ket{0} + d\ket{1}$. Then $\ket{v}\otimes\ket{w} = ac\ket{00}+ad\ket{01}+bc\ket{10}+bd\ket{11}$. But there is no choice of $a, b$ that makes this equal to $\ket{\Phi}$.

If $\ket{\psi_1} = \ket{\alpha_1}\ket{\beta_1}, \ket{\psi_2} = \ket{\alpha_2}\ket{\beta_2}$ are product vectors in $\mathcal{V}\otimes\mathcal{W}$. Then $\braket{\psi_1}{\psi_2} = \braket{\alpha_1}{\alpha_2}\braket{\beta_1}{\beta_2}$. In the case where one of the vectors is not a product vector, we have to write it as a sum of the basis vectors $\ket{e_i}\ket{f_j}$ of $\mathcal{V}\otimes\mathcal{W}$, and then expand that product out.

For example, if $\ket{A} = \sum_{i=1}^m \sum_{j=1}^n a_{ij}\ket{e_i}\ket{f_j}$, and $\ket{B} = \sum\sum b_{ij} \ket{e_i}\ket{f_j}$, then:
\begin{align*}
\braket{A}{B} &= \sum_{i,i'}\sum_{j,j'} a_{ij}^*b_{i'j'} \braket{e_i}{e_i'}\braket{f_j}{f_j'}\\
&= \sum_{i,j} a_{ij}^* b_{ij}
\end{align*} 

\section{Postulates of Quantum Mechanics}
\begin{enumerate}
\item[QM1] To any isolated/closed quantum system, one can associate a Hilbert space $\mathcal{V}$, called the \emph{state space}. The physical states are given by \emph{unit vectors} in $\mathcal{V}$. (More precisely, they are given by \emph{rays}: suppose we have $\ket{\psi}\in \mathcal{V}, \ket{\phi} = e^{\im\theta}\ket{\psi}$ for $\theta \in \R$, where we call $e^{\im\theta}$ a \emph{global phase factor}. We will see that there is no measurement that is able to distinguish $\ket{\phi}, \ket{\psi}$, so to all intents and purposes they are identical - they refer to the same state. Hence we only distinguish state vectors up to the equivalence relation of differing by a global phase factor, and call the equivalence classes the rays). We will be focusing on the specific case where $\mathcal{V} = \C^2$, with the states $a\ket{0} + b\ket{1}$ for $a,b \in \C$, called \emph{qubits}.

\item[QM2] Given a composite of two systems $S_1, S_2$ with vector spaces $\mathcal{V}_1, \mathcal{V}_2$, the state space of the composite system $S_1S_2$ is given by $\mathcal{V}_1\otimes \mathcal{V}_2$. Again, focusing on the qubit case, if we have $n$ cubits then the composite space of $S_1S_2\ldots S_n$ is $\mathcal{V} = (\C^2)^{\otimes n}$, with basis given by the bitstrings discussed above. Note that the number of coefficients required to define a member of $\mathcal{V}$ grows as $2^n$, the number of basis vectors, but the number of coefficients required to define a product state grows only linearly, as such a state can be written as $(a_1\ket{0}+b_1\ket{1})\otimes\ldots\otimes(a_n\ket{0}+b_n\ket{1})$.
\end{enumerate}

\subsection{Observables}
An \emph{observable} $A$ is a linear hermitian (or self-adjoint) operator that acts on the state space $\mathcal{V}$ of $S$. Recall that linearity means that $A(a\ket{v} + b\ket{w}) = aA\ket{v} + bA\ket{w}$, and in this case we can represent $A$ by some matrix with respect to a basis of $\mathcal{V}$. We have for each $A$ a \textit{unique} linear operator $A^{\dag}$, the \emph{adjoint} of $A$, with the property that $\braket{v}{Au} = \braket{A^{\dag}v}{u}$. From 1B linear algebra, we know $A^{\dag}$ has matrix given by the conjugate transpose of the matrix representing $A$ (when taken with the same basis of course). Hence to say that $A$ is \emph{self-adjoint} is to say that $A = A^{\dag}$.

An \emph{eigenvector} of $A$ is $\ket{phi}$ with the property that $A\ket{phi} = \lambda\ket{\phi}$ for $\lambda$ in the base field of $\mathcal{V}$. We call $\lambda$ an \emph{eigenvalue} of $A$. Some important examples of these linear maps will be the Pauli operators, given by:
\begin{align*}
I = \sigma_0 &= \begin{pmatrix}
1 & 0\\ 0 & 1
\end{pmatrix}\\
\sigma_x & = \begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix}\\
\sigma_y &= \begin{pmatrix}
0 & -\im \\ \im & 0
\end{pmatrix}\\
\sigma_z &= \begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix}
\end{align*}
Note that $\sigma_{i}^2 = I$, and $\sigma_x \sigma_y = \im \sigma_z$.

Moreover, $\sigma_x\ket{0} = \ket{1}, \sigma_x\ket{1} = \ket{0}$. We call $\sigma_x$ a \emph{bit flip} - it flips the value of a bit. The eigenvectors of $\sigma_x$ are $\ket{+}, \ket{-}$.

\hspace*{-1em}$\sigma_z\ket{0} = \ket{0}, \sigma_z\ket{1} = -\ket{1}$, so we call $\sigma_z$ a \emph{phase flip}. 

\hspace*{-1em}$\sigma_y\ket{0} = \im\ket{1}, \sigma_y\ket{1}=-\im\ket{0}$, and we call $\sigma_y$ a \emph{combined flip}.
\begin{figure}[H]
\centering
\begin{tabular}{c|cc}
& $\ket{0}$ & $\ket{1}$\\\hline
$\sigma_x$ & $\ket{1}$ & $\ket{0}$\\
$\sigma_y$ & $\im\ket{1}$ & $-\im\ket{0}$\\
$\sigma_z$ & $\ket{0}$ & $-\ket{1}$
\end{tabular}
\end{figure}
\subsection{Dirac Notation For Linear Operators/Maps}
A simple example of a linear operation on $\mathcal{V}$: let $M$ be the matrix given by $\ket{v}\bra{w}$ for $\ket{v},\ket{w} \in \C^2$. Then $M\ket{x} = \ket{v}\bra{w}\ket{v} = \braket{w}{x}\ket{v}$, so this is a rank 1 linear operation with image $\spann \ket{v}$. The kernel of $M$, $\ker M = \{\ket{a} : \ket{a} \bot \ket{\omega}\}$, since if $\ket{a}$ is perpendicular to $\ket{w}$ then $\braket{w}{a} = 0$.

In general, we can represent any linear operation $A:\C^2 \to \C^2$ by a $2\times 2$ matrix $\begin{pmatrix} a & b \\ c& d\end{pmatrix} = a \ket{0}\bra{0} + b\ket{0}\bra{1} + c \ket{1}\bra{0} + d \ket{1}\bra{1}$. In general, if $A : \mathcal{V} \to \mathcal{V}$ where $\dim \mathcal{V} = n$, we can write $A = \sum_{i,j}^n a_{ij}\ket{e_i}\bra{e_j}$, and so we have $A\ket{\psi} = \sum_{i,j,k} a_{ij}\psi_k \ket{e_i}\braket{e_j}{e_k} = \sum_{i,j} a_{ij}\psi_j\ket{e_i}$.

We define the \emph{trace} of a linear map to be the sum along the leading diagonal of a matrix representing the linear map, i.e. $\tr A = \sum_{i} \bra{e_i}A\ket{e_i}$. Note that $\braket{w}{v} = \tr \ket{v}\bra{w}$.

A very useful class of operators is the \emph{projection operators}, which are \emph{idempotent} (i.e. $\pi^2 = \pi$) hermitian operators. For example, $\Pi_v = \ket{v}\bra{v}$, a rank 1 projection operator, satisfies $\Pi_v^2 = \Pi_v, \Pi_v^{\dag} = \Pi_v$. This projection map returns only the component of the vector along the vector onto which it is a projection. We can generalise this to higher dimensional subspaces: if $\mathcal{E} \subseteq \mathcal{V}$, where $\mathcal{E} = \spann \{e_1, \ldots, e_d\}$, then we can define the projection onto $\mathcal{E} = \sum_{i=1}^d \Pi_{e_i}$.

\subsection{Tensor Products of Linear Operations}
If $a,B$ act on $\C^2$, then given $\ket{i}\ket{j} \in \C^2 \otimes \C^2$, we can define the tensor product of the linear operations $(A \otimes B)\ket{i}\ket{j}$ to be the $4 \times 4$ linear map $(A\otimes B)_{ij} = A_{\ceil{i/2}\ceil{j/2}}B_{i\%2, j\%2}$, i.e. $a_{ij}B$ for each $i,j$.

\begin{itemize}
\item[QM3] We can describe the time evolution of a \emph{closed} quantum system - we can think about how the states evolve by writing them as say $\ket{\psi(t_1)}, \ket{\psi(t_2)}$ for some time points $t_2 > t_1$. The third postulate says that this evolution is given by a \emph{unitary operator}, i.e. a linear map $U$ for which $UU^{\dag} = I = U^{\dag}U$, so that $\ket{\psi(t_2)} = U(t_1,t_2)\ket{\psi(t_2)}$.
\end{itemize}
\begin{proposition}
Let $U:\mathcal{V}\to \mathcal{V}$ be a linear operator. The following are equivalent:
\begin{enumerate}
\item $U$ is unitary
\item $U$ maps complete orthonormal bases to complete orthonormal bases
\item $U$ preserves inner products, so $\braket{Uv}{Uw} = \braket{v}{w}$
\item The columns of $U$ form an orthonormal basis of $\mathcal{V}$
\item The rows of $U$ form an orthonormal basis of $\mathcal{V}$
\end{enumerate}
\end{proposition}
\begin{proof}
Done in lectures but was a mess. Check this for yourself. (If you can be bothered).
\end{proof}

In Quantum Mechanics, one measures an \emph{observable}, say $A$. An important representation of $A$ is as \emph{spectral projection}: we can write $A = \sum_{n} a_n P_n$, where $a_n$ are the (real) eigenvalues of $A$, and $P_n$ are the spectral projection operators. In the case that $a_n$ is a \emph{non-degenerate} eigenvalue (i.e. has only one $\ket{\phi_n}$ with $A\ket{\phi_n} = a_n\ket{\phi_n}$), then $P_n = \ket{\phi_n}\bra{\phi_n}$. If $a_n$ is degenerate, say with  multiplicity $m > 1$, then  $P_n = \sum_{i=1}^m \ket{\phi_n^{(i)}}\bra{\phi_n^{(i)}}$, where the $\ket{phi_n^{(i)}}$ are the eigenvectors with $a_n$ as their eigenvalue.

The outcome of a measurement is one of the eigenvalues, $a_k$. If the \emph{initial state} of the system to be measured is $\ket{\phi} \in \mathcal{V}$, then $\P($outcome is $a_k) = p(a_k)$, where $p(a_k) = \bra{\phi}P_k\ket{\phi}$. If the outcome is $a_k$, then after the measurement, the state collapses to $\frac{P_k\ket{\phi}}{\sqrt{p(a_k)}}$. This is called the \emph{Born Rule.} Note that this measurement is completely specified by the projection operations. 

We can also measure \emph{relative to a basis}: given a basis $\mathcal{B} = \{\ket{e_i}\}_{i=1}^n$, and $\ket{\phi} \in \mathcal{V}$. Then the probability we observe outcome $j \in [n]$ is $\braket{\phi}{e_j}\braket{e_j}{\phi} = |a_j|^2$, and the state collapses to $\frac{a_j \ket{e_j}}{\sqrt{|a_j|^2}} = \ket{e_j}$.

For example, if $\mathcal{V} = \C^2$, and $\mathcal{B} = \{\ket{0},\ket{1}\}$, then we measure $\ket{\phi} = a\ket{0}+b\ket{1}$ relative to $\mathcal{B}$. The outcomes are $0$ or $1$. $\P(0) = \bra{\phi}P_0\ket{\phi} = \braket{\phi}{0}\braket{0}{\phi} = |a|^2$, and if we get $0$ then the state collapses to $\ket{0}$.

\subsection{Incomplete Projective Measurements}
In the previous case, where we have a complete orthonormal basis, we decomposed the space up into dimension 1 subspaces, i.e. the spans of the eigenvectors, and then used rank 1 projection operators onto these subspaces. We can instead, more generally, decompose $\mathcal{V}$ into mutually orthogonal subspace of dimension $\geq 1$, so that $\mathcal{V} = \bigoplus_{i=1}^d \mathcal{E}_i$, where $\sum_{i=1}^d \dim \mathcal{E}_i = \dim\mathcal{V} = n$. Then if $\Pi_i$ is the projection operator onto $\mathcal{E}_i$, we have $\Pi_j\Pi_k = \delta_{jk}\Pi_j$.

The incomplete measurement of $\ket{\psi} \in \mathcal{V}$ relative to the orthogonal decomposition $\{\mathcal{E}_1, \ldots, \mathcal{E}_d\}$ returns $i$ with probability $p(i) = \bra{\psi}\Pi_i\ket{\psi}$. If the outcome of the measurement is $i$, then the post measurement state is $\frac{\Pi_i \ket{\psi}}{\sqrt{p(i)}}$.

Given an incomplete measurement with decomposition $\{\mathcal{E}_i,\ldots, \mathcal{E}_d\}$ of $\mathcal{V}$, we can choose a basis $\{\ket{e^{(i)}_j}\}_{j=1}^{d_i}$ of each $\mathcal{E}_i$, so that together  they form an orthonormal basis of the whole space $\mathcal{V}$. Then the measurement will return $k$ (corresponding to the subspace $\mathcal{E}_k$) with probability $\sum_{i=1}^{d_k} \left(\bra{\psi} P_i^{(k)} \ket{\psi}\right) = \bra{\psi} \left(\sum_{i=1}^{d_k} P_i^{(k)}\right) \ket{\psi} = \bra{\psi} \Pi_{k} \ket{\psi}$.

For example, if we want to measure the \emph{parity} of a 2-bit string $b_1b_2$ (i.e. $b_1$ \textsc{xor} $b_2$), we might have some state given by $\ket{\psi} = a\ket{00} + b\ket{01} + c\ket{10} + d\ket{11}$. Then the space of 0 parity, $\mathcal{E}_0 = \spann\{\ket{00},\ket{11}\}$, and likewise we have $\mathcal{E}_1 = \spann\{\ket{01},\ket{11}\}$. Then $\P(0) = \bra{\psi}\Pi_0 \ket{\psi} = \bra{\psi}\Big(\ket{00}\bra{00}+\ket{11}\bra{11}\Big)\ket{\psi} = \braket{\psi}{00}\braket{00}{\psi} +  \braket{\psi}{11}\braket{11}{\psi} = |a|^2 + |d|^2$, and the post measurement state is $\frac{a\ket{00} + d\ket{11}}{\sqrt{|a|^2 + |d|^2}}$.

We can also do this on the vector level: $\P(0) = \P(00) + \P(11) = \braket{\psi}{00}\braket{00}{\psi} + \braket{\psi}{11}\braket{11}{\psi} = |a|^2+|d|^2$. It is not a coincidence that we get the same answer, but of course since this isn't a pure course we won't formalise this into a nice little lemma or proposition, and instead we'll just assert it's obvious and move on.


\end{document}
