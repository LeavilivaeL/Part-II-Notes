\documentclass[10pt,a4paper]{article}
\input{./preamble.tex}

\title{Coding \& Cryptography}
\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Communication Channels}
This course will be about modelling communication. In general, we have the following idea:

\begin{figure}[H]
\centering
\begin{tikzpicture}
\node (s) at (0,0) {\textsc{Source}};
\node (e) at (4,0) {\textsc{Encoder}};
\node (d) at (8,0) {\textsc{Decoder}};
\node (r) at (12,0) {\textsc{Receiver}};
\draw (s) edge[->] (e) (d) edge[->] (r);
\draw[->, decorate, line join=round, decoration={zigzag, segment length=4, amplitude=0.9, post=lineto, post length=2pt}] (e) -- node[above] {\textsc{Channel}} node[below] {Errors, Noise} (d) ;
\end{tikzpicture}
\end{figure}
\vspace{-1em}
For example, the channel might be an optical or electrical telegraph, modems, audio CDs, satellite relays. The encoding and decoding might be something like \textsc{Ascii}, so that each character in the email ``Call at 2pm" would be encoded into 8 bits using $a = 1100101, \ldots$, giving an 84 bit message to be transmitted via the internet, and decoded by the receiver's email client. Our general aim here will be, given some source and channel (modelled probabilistically), to design an encoder and decoder to send messages economically and reliably.

\underline{Examples}
\begin{itemize}
\item (Noiseless coding) Morse Code. In this code, more common letters are assigned shorter codes, so that we have $A = \cdot - \ast, E = \cdot \ast, Q = - - \cdot - \ast, Z = - - \cdot \cdot \ast$. This is adapted to the \textit{source}, in the sense that we chose the codes based off the expected distribution of letters that we will have to transmit.

\item (Noisy coding) \textsc{Isbn}. In the \textsc{Isbn} encoding, every book is given a 10 digit number $a_1a_2\ldots a_{10}$, with $\sum_{i=1}^{10} (11-i)a_i \equiv 0 \mod 11$. This is adapted to the \textit{channel}, in the sense that the likely errors to occur will be 1 incorrect digit, or accidentally transposing two digits, which this code is resistant to (will return an error rather than an erroneous result).
\end{itemize}

A \emph{communication channel} accepts symbols from some alphabet $\mathscr{A} = \{a_1, a_2, \ldots, a_r\}$ (e.g. $\{0,1\}, \{a,b,\ldots,z\}$), and outputs symbols from an alphabet $\mathscr{B} = \{b_1, \ldots, b_s\}$. The channel is modelled by the probabilities:
\begin{center}$\P(y_1,y_2,\ldots,y_n$ received$| x_1,x_2,\ldots,x_n$ sent$) = \prod_{i=1}^{n} \P(y_i$ received$| x_i$ sent$)$\end{center}

A \emph{discrete memoryless channel (DMC)} is a channel with $p_{ij} = \P(b_j$ received$|a_i$ sent$)$ the same for each channel usage and independent of any past or future channel usages.

The \emph{channel matrix} is $P = (p_{ij})$, an $r \times s$ stochastic matrix.

\underline{Examples:} The \emph{binary symmetric channel (BSC)} with error probability $p \in [0,1]$ has $\mathscr{A} = \mathscr{B} = \{0,1\}$. The channel matrix is $\begin{pmatrix} 1-p & p \\ p & 1-p \end{pmatrix}$. A symbol is transmitted correctly with probability $1-p$.

The \emph{binary erasure channel} has input alphabet $\{0,1\}$, and output alphabet $\{0, 1, \ast\}$, where we miss a bit is probability $p$, giving channel matrix $\begin{pmatrix} 1-p & 0 & p \\ 0 & 1-p & 0\end{pmatrix}$. We can model $n$ uses of a channel by the \emph{n\th extension} with input alphabet $\mathscr{A}^n$, and output alphabet $\mathscr{B}^n$.

A \emph{code} $c$ of \emph{length} $n$ is a function $c: M \to \mathscr{A}^n$ where $M$ is the set of all possible messages. Implicitly, we also have a decoding rule $\mathscr{B}^n \to M$.\\
The \emph{size} of $c$ is $m = |M|$. \\
The \emph{information rate} is $\rho(c) = \frac{1}{n}\log_2(m)$. \\
The \emph{error rate} is $\hat{e}(c) = \max_{x\in M} \{\P($error$|x$ sent$)\}$.

A channel can transmit reliably at a rate $R$ if there exists a sequence of codes $(c_n: n\geq 1)$ with $c_n$ a code of length $n$, $\lim_{n\to \infty}(\rho(c_n)) =R, \lim_{n\to\infty}(\hat{e}(c_n)) = 0$. The capacity of a channel is the supremum of all reliable transmission rates.

\begin{theorem}
A $BSC$ with error probability $p < \frac12$ has a non-zero capacity (i.e. good codes exist).
\end{theorem}
\begin{proof}
See \textbf{9.3}
\end{proof}

\section{Noiseless Coding}
\subsection{Prefix-free Codes}
For an alphabet $\mathscr{A}, |\mathscr{A}|<\infty$, let $\mathscr{A}^{\ast} = \bigcup_{n\geq 0} \mathscr{A}^n$, the set of all finite strings from $\mathscr{A}$. The \emph{concatenation} of strings $x = x_1\ldots x_r$ and $y = y_1\ldots y_s$ is $xy = x_1\ldots x_ry_1\ldots y_s$.

Let $\mathscr{A}, \mathscr{B}$, be alphabets. A \emph{code} is a function $c:\mathscr{A} \to \mathscr{B}^{\ast}$. The strings $c(a)$ for $a \in \mathscr{A}$ are called \emph{codewords} (cws). If $x, y \in \mathscr{B}^{\ast}$ then $x$ is a \emph{prefix} of $y$ if $y=xz$ for some $z \in \mathscr{B}^{\ast}$.

For example, we have the Greek fire code, found in the writings of Polybius around 280 BC. $\mathscr{A} = \{\alpha, \beta, \ldots, \omega\}, \mathscr{B} = \{1,2,3,4,5\}$, with code $\alpha \mapsto 11, \beta \mapsto 12, \ldots, \psi \mapsto 53, \omega \mapsto, 54$, where $xy$ means ``$x$ torches held up, and another $y$ torches nearby".

The English language is even a code: we can let $\mathscr{A}$ be words in a given dictionary, and $\mathscr{B} = \{a,b,\ldots,z,\text{\textvisiblespace}\}$, where the coding function is to spell the word and follow it with a space.

We send a message $x_1\ldots x_n \in \mathscr{A}^{\ast}$ as $c(x_1)\ldots c(x_n) \in \mathscr{B}^{\ast}$. So $c$ extends to a function $c^{\ast}:\mathscr{A}^{\ast} \to \mathscr{B}^{\ast}$.

$c$ is \emph{decipherable}/\emph{decidable} if $c^{\ast}$ is injective, so that each string in $\mathscr{B}^{\ast}$ could have come from at most one message. Note that it isn't sufficient to just have $c$ injective, although clearly this is necessary:

$\mathscr{A} = \{1,2,3,4\}, \mathscr{B} = \{0,1\}, c:1\mapsto 0, 2\mapsto 1, 3\mapsto 00, 4\mapsto 01$. Then $c^{\ast}(114) = 0001 = c^{\ast}(312)$.

If $|\mathscr{A}|=m, |\mathscr{B}|=a$, then we say $c$ is an \emph{a-ary code of size m}. 2-ary = \emph{binary}, 3-ary=\emph{ternary}.

We aim to construct decipherable codes with short word lengths. Assuming $c$ is injective, the following are always decipherable:
\begin{itemize}
\item Block codes, where every codeword has the same length (e.g. Greek fire, \textsc{Ascii})
\item Comma codes, where we have an ``end of word" character (e.g. English language)
\item Prefix-free codes, where no codeword is a prefix of any other distinct words.
\end{itemize}
Note that both of the first two are special cases of prefix-free codes. Prefix-free codes are often called \emph{instantaneous} or \emph{self-punctuating} codes. Note that not all decipherable codes are prefix-free: $0\mapsto 01, 1 \mapsto 011$ is decipherable but not prefix free.

\begin{theorem}[Kraft's Inequality]
Let $|\mathscr{A}| = m, |\mathscr{B}| = a$. A prefix-free code $c:\mathscr{A} \to \mathscr{B}^{\ast}$ with word lengths $\ell_1, \ldots, \ell_m$ exists if and only if:
\begin{align*}
\sum_{i=1}^{m} a^{-\ell_i} &\leq 1 \tag{\ast}
\end{align*}
\end{theorem}
\begin{proof}
Rewrite (\ast) as $\sum_{\ell=1}^s n_{\ell}a^{-\ell} \leq 1 (\star)$, where $n_{\ell}$ is the number of codewords of length $\ell$ and $s = \max_{1 \leq i \leq m} \ell_i$.

\begin{itemize}
\item[$\implies$] If $c:\mathscr{A} \to \mathscr{B}^{\ast}$ is prefix-free, then $n_1 a^{s-1} + n_2 a^{s-2} + \ldots n_s \leq a^s$, since the \textsc{Lhs} is the number of strings of length $s$ in $\mathscr{B}$ with some codeword of $c$ as a prefix, and \textsc{Rhs} is the number of strings of length $s$. Dividing by $a^{s}$ gives $(\star)$.

\item[$\impliedby$] Given $n_1, \ldots, n_s$ satisfying $(\star)$, we need to construct a prefix-free code $c$ with $n_\ell$ codewords of length $\ell$ for all $\ell \leq s$. We use induction on $s$. The case $s=1$ is clear: we have $(\star)$ gives $n_1 \leq a$, so we can choose a code.

By the induction hypothesis there is a prefix-free code $\hat{c}$ with $n_{\ell}$ codewords of length $\ell$ for all $\ell \leq s-1$. Then $(\star)$ gives:
\begin{align*}
n_1 a^{s-1} + n_2 a^{s-2} + \ldots + n_{s-1}a + n_s &\leq a^s
\end{align*}
where the first $s-1$ terms on \textsc{Lhs} sum to the number of strings of length $s$ with some codeword of $\hat{c}$ as a prefix, and the \textsc{Rhs} is the number of strings of length $s$. Hence we can add at least $n_s$ new codewords of length $s$ to $\hat{c}$ and maintain the prefix-free property, giving our code.
\end{itemize}
\end{proof}

\begin{theorem}[McMillan]
Any decipherable code satisfies Kraft's inequality
\end{theorem}
\begin{proof}[Proof (Karush).]
Let $c:\mathscr{A} \to \mathscr{B}^{\ast}$ be a decipherable code with codewords of lengths $\ell_1, \ldots, \ell_m$. Let $s = \max_{1 \leq i\leq m} \ell_i$. Then for $R \in \N:$
\begin{align*}
\left(\sum_{i=1}^{m} a^{-\ell_i}\right)^R = \sum_{l=1}^{Rs} b_{\ell}a^{-\ell}
\end{align*}
where $b_{\ell} = |\{x \in \mathscr{A}^R : c^{\ast}(x)$ has length $\ell\}| \leq |\mathscr{B}^{\ell}| = a^{\ell}$, using the fact that $c^{\ast}$ is injective. Then:
\begin{align*}
\left(\sum_{i=1}^m a^{-\ell_i}\right)^R &\leq \sum_{l=1}^R a^\ell a^{-\ell} = Rs\\
\sum_{i=1}^m a^{-\ell_i} &\leq (Rs)^{\frac1R} \to 1 \text{ as } R \to \infty
\end{align*}
\end{proof}

\begin{corollary}
A decipherable code with prescribed word lengths exists iff a prefix-free code with same word lengths exists.
\end{corollary}
\begin{proof}
\item
\begin{itemize}
\item[$\implies$] Use \textbf{1.2} to generate a prefix-free code by \textbf{1.1}
\item[$\impliedby$] Prefix-free codes are decipherable.
\end{itemize}
\end{proof}

\section{Shannon's Noiseless Coding Theorem}
Entropy is a measure of `randomness' or `uncertainty'. Suppose we have a random variable $X$ that takes values $x_1, \ldots, x_n$ with probabilities $p_1, \ldots, p_n$. Then the \emph{entropy} (roughly speaking) is the expected number of fair coin tosses needed to simulate $X$.

\hspace*{-1em}\underline{Examples:}
\begin{itemize}
\item $p_1 = p_2 = p_3 = p_4 = \frac14$. We can identify $\{x_1, x_2, x_3, x_4\}$ with $\{HH, HT, TH, TT\}$, and so the entropy of this random variable is $2$.

\item $(p_1, p_2, p_3, p_4)=(\frac12,\frac14,\frac18,\frac18)$. Here, the entropy is $1\cdot\frac12+2\cdot\frac14+3\cdot\frac18+3\cdot\frac18 = \frac74$. We might say then, since the entropy is greater, that the first example is ``more random" than the second.
\end{itemize}

More concretely, the \emph{Shannon (or information) entropy} of $X$ is $H(X) = -\sum_{i=1}^n p_i \log_2 p_i$. Note that $H(X) \geq 0$ with equality if and only if $\P(X=x_i) = 1$. This is measured in \emph{bits}. We take the convention that $0\log 0 =0$.

\hspace*{-1em}\underline{Example}: Consider a biased coin, where $\P(X=H) = p, \P(X=T) = 1-p$. Then $H(X) = -p\log p-(1-p)\log(1-p) = p\log(\frac{1-p}{p} - \log(1-p)$.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[xmin=0, xmax=1 ,ymax=1.5, samples=50, xlabel = {$p$}, ylabel={$H(X)$}]
\addplot[blue, ultra thick, samples=200] (x, {x*log2((1-x)/x) - log2(1-x)});
\end{axis}
\end{tikzpicture}
\end{figure}

\begin{proposition}[Gibb's Inequality]
Let $(p_1, \ldots, p_n)$ and $(q_1, \ldots, q_n)$ be probability distributions. Then:
\begin{align*}
-\sum_{i=1}^n p_i \log p_i \leq -\sum_{i=1}^n p_i \log q_i
\end{align*}
with equality if and only if $p_i = q_i$ for all $i$.
\end{proposition}

\begin{proof}
Since $\log x = \frac{\ln x}{\ln 2}$, we may replace $\log$ by $\ln$ in the proof. Put $I = \{1 \leq r\leq n: p_i \neq 0\}$. Now $\ln x \leq x-1$ with equality if and only if $x=1$. So we have $\ln \frac{q_i}{p_i} \leq \frac{q_i}{p_i} - 1$, and hence:
\begin{align*}
\sum_{i \in I} p_i \ln \frac{q_i}{p_i} &\leq \sum_{i\in I}q_i - \sum_{i \in I}p_i\\
&= \sum_{i \in I} q_i - 1 \leq 0\\
\therefore - \sum_{i \in I}p_i \ln p_i &\leq - \sum_{i \in I} p_i \ln q_i\\
\therefore -\sum_{i=1}^n p_i \log p_i &\leq -\sum_{i=1}^n p_i \log q_i
\end{align*}
If equality holds, then $\sum_{i\in I} p_i =1$ and $\frac{p_i}{q_i} = 1$ for all $i \in I$, so $p_i = q_i$
\end{proof}

\begin{corollary}
$H(p_1, \ldots, p_n) \leq \log n$ with equality if and only if $p_1 = \ldots = p_n = \frac1n$.
\end{corollary}
\begin{proof}
Take $q_1 = \ldots = q_n = \frac1n$ in \textbf{2.1}.
\end{proof}

Let $\mathscr{A} = \{\mu_1, \ldots, \mu_m\}$, and $|\mathscr{B}| = a$, where $m, a\geq 2$. The random variable $X$ takes values $\mu_1, \ldots, \mu_m$ with probabilities $p_1, \ldots, p_m$. We say a code $c:\mathscr{A} \to \mathscr{B}^{\ast}$ is \emph{optimal} if it is a decipherable code with smallest possible expected word length, $\E S = \sum_i p_i \ell_i$.

\begin{theorem}[Shannon's Noiseless Coding Theorem]
The expected word length $\E S$ of an optimal code satisfies:
\begin{align*}
\frac{H(X)}{\log a} \leq \E S < \frac{H(X)}{\log a}+1
\end{align*}
\end{theorem}
\begin{proof}
For the lower bound, take $c: \mathscr{A} \to \mathscr{B}^{\ast}$ decipherable with word lengths $\ell_1, \ldots, \ell_m$. Then set $q_i = \frac{a^{-\ell_i}}{D}$ where $D = \sum_{i=1}^m a^{-\ell_i}$. Now we have that $\sum_{i=1}^m q_i = 1$. By Gibbs,
\begin{align*}
H(X) &\leq -\sum_{i=1}^m p_i \log q_i\\
&= -\sum p_i\left(-\ell_i \log a - \log D\right)\\
&= \left(\sum_{i=1}^m p_i \ell_i\right) \log a + \log D
\end{align*}
By McMillan, $D \leq 1$, so $\log D \leq 0$, and so $H(X) \leq \left(\sum_{i=1}^m p_i \ell_i\right) \log a = \E S \cdot \log a$, and we have equality if and only if $p_i = a^{-\ell_i}$ for some integers $\ell_1, \ldots \ell_m$.

For the upper bound, take $\ell_i = \ceil{-\log_a p_i}$. Then $-\log_a p_i \leq \ell_i \implies p_i \geq a^{-\ell_i}$.

Now $\sum_{i=1}^m a^{-\ell_i} \leq \sum_{i=1}^m p_i = 1$. By Kraft, there is some prefix-free code $c$ with word lengths $\ell_1, \ldots, \ell_m$, and the expected word length of $c$ is $\E S = \sum p_i \ell_i < \sum p_i (-\log_a p_i + 1) = \frac{H(X)}{\log a} + 1$.
\end{proof}

\hspace*{-1em}\underline{Example:} \emph{Shannon-Fano coding}\\
We mimic the above proof: given probabilities $p_1, \ldots, p_n$, set $\ell_i = \ceil{-\log_a p_i}$. Construct the prefix-free code with word lengths $\ell_1, \ldots, \ell_m$ by choosing in order of increasing length, ensuring that previous codewords are not prefixes. For example, if $a = 2, m = 5$ we have:\\
\begin{figure}[H]
\centering
\begin{tabular}{c|c|c|c}
$i$ & $p_i$ & $\ceil{-\log_2 p_i}$ & Codewords \\\hline
1 & 0.4 & 2 & 00 \\
2 & 0.2 & 3 & 010\\
3 & 0.2 & 3 & 011\\
4 & 0.1 & 4 & 1000\\
5 & 0.1 & 4 & 1001
\end{tabular}
\end{figure}
$\E S = \sum p_i \ell_i = 2.8$, entropy $= 2.12$

\section{Huffman Coding Algorithm}
Huffman was a student of Fano, and was thinking about how to construct an optimal code. For simplicity, we will take $a = 2$. Suppose we get messages with orders $p_1 \geq p_2 \geq \ldots \geq p_m$. Huffman gave a recursive definition of codes that we can prove are optimal. If $m=2$, then take codewords $0$ and $1$.

If $m > 2$, we first have a Huffman code for messages $\mu_1, \ldots, \mu_{m-2}, \nu$ with probabilities $p_1, \ldots, p_{m-2}, p_{m-1}+p_m$, then append $0$ and $1$ to give codewords for $\mu_{m-1}$ and $\mu_m$.

\hspace*{-1em}Note:
\begin{itemize}
\item Huffman codes are prefix-free.
\item We have some choices to make if some of the $p_j$ are equal, so Huffman codes are not unique.
\end{itemize}
\hspace*{-1em}\underline{Example:} Reconsider the previous example:
\begin{figure}[H]
\begin{subfigure}{0.5\textwidth}
\begin{tikzpicture}
\node[draw, circle] (a) at (0,10) {$p_i$};
\node[draw, circle] (b) at (-1,9) {0.6};
\node[draw, circle] (c) at (1,9) {0.4};
\node[draw, circle] (d) at (0,8) {0.4};
\node[draw, circle] (e) at (-2,8) {0.2};
\node[draw, circle] (f) at (1,7) {0.2};
\node[draw, circle] (g) at (-1,7) {0.2};
\node[draw, circle] (h) at (0,6) {0.1};
\node[draw, circle] (i) at (-2,6) {0.1};

\draw (a) -- node[above left] {0} (b) (a) -- node[above right] {1} (c);
\draw (b) -- node[above left] {0} (e) (b) -- node[above right] {1} (d);
\draw (d) -- node[above left] {0} (g) (d) -- node[above right] {1} (f);
\draw (g) -- node[above left] {0} (i) (g) -- node[above right] {1} (h);
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\begin{tabular}{c|c|c}
$i$ & $p_i$ & Codewords \\\hline
1 & 0.4 & 1\\
2 & 0.2 & 00\\
3 & 0.2 & 011\\
4 & 0.1 & 0100\\
5 & 0.1 & 0101
\end{tabular}
\end{subfigure}
\end{figure}
This code has expected length $2.2$, which is less than Shannon-Fano gave.
\begin{theorem}[Huffman, 1952]
Huffman codes are optimal.
\end{theorem}
\begin{proof}
We show this by induction on $m$. The case of $m=2$ is trivial. For $m > 2$, let $c_m$ be a Huffman code for source $X_m$ which takes values $\mu_1, \ldots, \mu_m$ with probabilities $p_1 \geq \ldots \geq p_m$. Then $c_{m-1}$ is constructed from a Huffman code $c_{m-1}$ for values $\mu_1, \ldots, \mu_{m-1}, \nu$ with probabilities $p_1, \ldots, p_{m-2}, p_{m-1}+p_m$.

Observe that $\E S_m = \E S_{m-1} + p_{m-1} + p_m$ by construction of $c_m$ from $c_{m-1}$.

Now let $c_m'$ be an optimal code for $X_m$. Without loss of generality, we may take $c_m'$ to be prefix-free and the last two codewords of $c_m'$ have maximal length and differ only in the last digit (see \textbf{3.2} below). Say $c_m'(\mu_{m-1}) = y0, c_m'(\mu_m) = y1$ for some $y \in \{0, 1\}^\ast$.

Let $c_{m-1}'$ be the prefix free code for $X_{m-1}$ given by $c_{m-1}'(\mu_i) = c_m'(\mu_i), c_{m-1}'(\nu) = y$.

Then the expected word length is $\E S_m' = \E S_{m-1}' + p_{m-1} + p_m \geq \E S_{m-1} + p_{m-1} + p_m = \E S_m$ by the inductive hypothesis, and so $c_m$ is optimal.
\end{proof}
\begin{lemma}
Suppose messages $\mu_1, \ldots, \mu_m$ are sent with probabilities $p_1,\ldots, p_m$, with an optimal code $c$ with word lengths $\ell_1, \ldots, \ell_m$. Then:
\begin{enumerate}
\item If $p_i > p_j$ then $\ell_i \leq \ell_j$.
\item Among all codewords of maximal length, there are two that differ only in the last digit.
\end{enumerate}
\end{lemma}
\begin{proof}
Otherwise, modify $c$ by swapping the $i\th$ and $j\th$ codewords, or deleting the last letter of each codeword of maximal length. The modified code is still prefix-free but has shorter expected word length, contradicting optimality of $c$.
\end{proof}
\section{Joint Entropy}
If $X,Y$ are random variables with value in $\mathscr{A}$ and $\mathscr{B}$. Then $(X,Y)$ is also a random variable with entropy $H(X,Y)$, the \emph{joint entropy} of $X,Y$.
\begin{align*}
H(X,Y) = -\sum_{x \in \mathscr{A}} \sum_{y \in \mathscr{B}} \P(X=x, Y=y)\log \P(X=x, Y=y)
\end{align*}
We can of course generalise this to any finite number of random variables. We will use Gibb's (\textbf{2.1}) to prove:
\begin{lemma}
Let $X,Y$ be random variables taking values in $\mathscr{A}, \mathscr{B}$. Then:\begin{align*}
H(X,Y) \leq H(X) + H(Y)
\end{align*}
with equality if and only if $X$ and $Y$ are independent.
\end{lemma}
\begin{proof}
Let $\mathscr{A} = \{x_1, \ldots, x_m\}, \mathscr{B} = \{y_1, \ldots, y_n\}$. Set $p_{ij} = \P(X=x_i, Y=y_i), p_i = \P(X=x_i), q_i = \P(Y=y_i)$. Then Gibb's inequality with $\{p_{ij}\}$ and $\{p_iq_j\}$ gives:
\begin{align*}
-\sum_{i,j} p_{ij}\log p_{ij} \leq -\sum_{i,j} p_{ij}\log(p_iq_j) &= -\sum_i\left(\sum_j p_{ij}\right) \log p_i - \sum_j \left(\sum_i p_{ij}\right) \log q_j\\
&= -\sum_i p_i \log p_i -\sum_j q_j \log q_j
\end{align*}
i.e. $H(X,Y) \leq H(X) + H(Y)$, with equality if and only if $p_{ij} = p_i q_j$ for all $i, j$, i.e. when $X,Y$ are independent.
\end{proof}

\hspace*{-1em}\underline{Example:} Let $X$ be a random variable that takes $D$ values with probability $\frac1D$. Then $H(X) = \log_2(D)$. Suppose $X_1, \ldots, X_N$ are i.i.d. with the same distribution as $X$. Then $H(X_1, \ldots, X_N) = N \log_2 D$.

\section{Error Correcting Codes}
\subsection{Noisy Channels and Hamming's Code}
A \emph{binary [n,m]-code} is a subset $C \subseteq \{0,1\}^n$ of \emph{size} $m = |C|$, \emph{length} $n$. The elements of $C$ are called \emph{codewords}. We use an $[n,m]$-code to send one of $m$ messages through a binary symmetric channel, making $n$ uses of the channel. Clearly $1 \leq m \leq 2^n$, so $0\leq \frac{1}{n}\log m \leq 1$. If $|C| = 1$ then $\rho(C) = 0$, and if $C= \{0,1\}^n$ then $\rho(C) = 1$.

For $x, y \in \{0,1\}^n$, the \emph{Hamming distance} $d(x,y) = |\{i:1\leq i\leq n, x_i \neq y_i \}|$, i.e. the number of positions where $x$ and $y$ differ.

We have three possible decoding rules:
\begin{enumerate}
\item The \emph{ideal observer} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ maximising $\P(c$ sent$|x$ received$)$.
\item Then \emph{maximum likelihood} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ maximising $\P(x$ received$|c$ sent$)$.
\item The \emph{minimum distance} decoding rule decodes $x \in \{0,1\}^n$ as $c\in C$ minimising $d(x,c)$.
\end{enumerate}

\begin{lemma}
\item
\begin{enumerate}
\item If all the messages are equally likely, then 1. and 2. agree.
\item If $p < \frac{1}{2}$, then 2. and 3. agree.
\end{enumerate}
\end{lemma}
\begin{proof}
\item
\begin{enumerate}
\item By Bayes' Rule:
\begin{align*}
\P(c\sent |x \received) = \frac{\P(c \sent)}{\P(x\received)}\P(x\received|c\sent)
\end{align*}
\end{enumerate}
By Hypothesis, $\P(c\sent)$ is independent of $c \in C$, and so for fixed $x$, maximising $\P(c\sent|x\received)$ is the same as maximising $\P(x\received|c\sent)$.

\item Let $r = d(x,c)$. Then $\P(x\received|c\sent) = p^r(1-p)^{n-r} = (1-p)^n\left(\frac{p}{1-p}\right)^r$. Since $p < \frac12$, $\frac{p}{1-p} < 1$, and so maximising $\P(x\received|c\sent)$ is the same as minimising $d(x, c)$.
\end{proof}

For instance, suppose $000$ is sent with probability $\frac{9}{10}$, and $111$ with probability $\frac{1}{10}$, through a binary symmetric channel with error probability $\frac14$. If we receive $110$, the ideal receiver computes $\P(000\sent|110\received)=\frac34; \P(110\sent|110\received)=\frac14$, and so decodes it as $000$. But the minimum distance (and so maximal likelihood) code is $111$. Henceforth, we will decide to use minimal distance decoding. 

Note that minimal distance decoding can be expensive in terms of time and storage if $|C$ is large, and we also need to specify a convention in the case of a tie (e.g. make a random choice, request the message again).

A code is \emph{d-error detecting} if changing up to $d$ digits in each codeword can never produce another codeword. It is \emph{e-error correcting} if, knowing that $x \in \{0,1\}^n$ differs from some codeword in at most $e$ places, we can deduce uniquely what the codeword is.

\hspace*{-1em}\underline{Examples}
\begin{enumerate}
\item A \emph{repetition code} of length $n$ has codewords $00\ldots0, 11\ldots1$. This is an $[n,2]$-code. It is $(n-1)$ error detecting and $\floor{\frac{n-1}{2}}$-error correcting. But the information rate is only $\frac1n$.
\item A \emph{simple parity check code} or \emph{paper tape code}: identify $\{0,1\}$ with $\F_2$ (i.e. arithmetic modulo 2), and let $C = \{(x_1,\ldots, x_n)\in \{0,1\}^n : \sum x_i = 0\}$. This is an $[n, 2^{n-1}]$-code. It is 1-error detecting, but cannot correct errors. Its information rate is $\frac{n-1}{n}$.
\item \emph{Hamming's Original Code} is a $2$-error detecting and 1-error correcting binary [7,16]-code:
\begin{align*}
C = \left.\begin{cases} & c_1+c_3+c_5+c_7 = 0\\ c \in \F_2^7 : & c_2+c_3+c_6+c_7 = 0\\ &c_4+c_5+c_6+c_7 = 0\end{cases}\right\}
\end{align*}
\end{enumerate}
The bits $c_3,c_5,c_6,c_7$ are arbitrary and $c_1, c_2, c_4$ are forced. The information rate is $\frac47$.

Given $x \in \F_2^7$, we form the \emph{syndrome} $z = (z_1,z_2,z_4) \in \F_2^7$, where $z_1 = x_1+x_3+x_5+x_7,$ $z_2 = x_2+x_3+x_6+x_7,$ $z_4 = x_4+x_5+x_6+x_7$. If $x \in C$ then $z = (0,0,0)$. If $d(x,c) = 1$ for some $c \in C$ then $x_i$ and $c_i$ differ for $i = z_1 + 2z_2 + 4z_4$. This can be checked easily for $c=0$ with a case by case check of the seven binary sequences of six 0s and one 1, e.g. $x=0010000$ gives a syndrome $z = (1,1,0), i = 1+2+0 = 3$.

\begin{lemma}
$d$ is a metric on $\F_2^n$.
\end{lemma}
\begin{proof}
Immediately, $d(x,y) \geq 0$, with equality if and only if $x=y$, and $d(x,y)=d(y,x)$. For the triangle inequality, note that if $x$ and $z$ differ at position $i$ then either $x,y$ differ at $i$ or $y,z$ differ at $i$. So every difference appearing in $d(x,z)$ appears in $d(x,y)+d(y,z)$, so $d(x,z) \leq d(x,y)+d(y,z)$.
\end{proof}

Note that $d(x,y) = \sum_i d_1(x_i,y_i)$ where $d_1$ is the discrete metric on $\F_2$. We define the \emph{minimum distance} of a code to be $\min_{c_1 \neq c_2} d(c_1,c_2)$.

\begin{lemma}
Let $C$ have minimal distance $d$. Then:
\begin{enumerate}
\item $C$ is $(d-1)$-error detecting, but cannot detect all sets of $d$ errors.
\item $C$ is $\floor{\frac{d-1}{2}}$-error correcting, but cannot correct all sets of $\floor{\frac{d-1}{2}}+1$ errors.
\end{enumerate}
\end{lemma}
\begin{proof}
\item
\begin{enumerate}
\item $d(c_1,c_2) \geq d$ for all distinct $c_1, c_2 \in C$. So $C$ is $(d-1)$-error detecting. But $d(c_1, c_2)=d$ for some $c_1, c_2 \in C$. So $C$ cannot detect all sets of errors.

\item Define the closed Hamming ball with center $x \in \F_2^n$, radius $r\geq 0$ as $B(x,r) = \{y \in \F_2^n: d(x,y)\leq r\}$. Now $C$ is $e$-error correcting if and only if, for all $c_1 \neq c_2 \in C$, we have $B(c_1, e) \cap B(c_2, e) = \emptyset$.

If $x \in B(c_1,e) \cap B(c_2,e )$, then $d(c_1,c_2) \leq d(c_1,x)+d(x,c_2) \leq 2e$. So if $d \geq 2e+1$, then $C$ is $e$-error correcting, with $e = \floor{\frac{d-1}{2}}$. For the second part, take $c_1, c_2 \in C$ with $d(c_1, c_2) = d$. Then suppose $x \in \F_2^n$ differs from $c_1$ in $e$ digits where $c_1, c_2$ differ too. Then $d(x,c_1) = e, d(x,c_2) = d-e$. If $d < 2e$ then $B(c_1, e) \cap B(c_2, d-e) \neq 0$, and so $C$ cannot correct all sets of $e$-errors. Then take $e = \ceil{\frac{d}{2}} = \floor{d-1}{2}+1$.
\end{enumerate}
\end{proof}
As a point of a notation, an $[n,m]$-code with minimum distance $d$ will be denoted as an $[n,m,d]$-code.

\hspace*{-1em}\underline{Examples:}
\begin{enumerate}
\item Repetition of length $n$ is an $[n,2,n]$-code.
\item Simple parity check code of length $n$ is an $[n, 2^{n-1}, 2]$-code.
\item Hamming's code is 1-error correcting, so $d\geq 3$. 0000000, 1110000 are both codewords, so it is a $[7,16,3]$-code, and hence 2-error correcting.
\end{enumerate}

\section{Covering Estimates}
Denote $V(n,r) = |B(x,r)| = \sum_{i=0}^r \binom{n}{i}$, independent of $x \in \F_2^n$, as the \emph{volume} of the ball (i.e. the number of points it contains).

\begin{lemma}[Hamming's Bound]
An $e$-error correcting code of length $n$ has:
\begin{align*}
|C| \leq \frac{2^n}{V(n,e)}
\end{align*}
\end{lemma}
\begin{proof}
Suppose $C$ is $e$-error correcting. Then $B(c_1, e) \cap B(c_2, e) = \emptyset$ for all $c_1 \neq c_2 \in C$. Then $\sum_{c \in C} |B(c,e)| \leq |\F_2^n| = 2^n$, i.e. $|C|V(n,e) \leq 2^n$.
\end{proof}
A code $C$ of length $n$ that can correct $e$ errors is \emph{perfect} if $|C|=\frac{2^n}{V(n,e)}$. Equivalently, a code is perfect if for all $x \in \F_2^n$ there is a unique $c \in C$ such that $d(x,c) \leq e$, or $\F_2^n = \bigcup_{c\in C}B(c,e)$, i.e. any $e+1$ errors will make you decode incorrectly.

For example, Hamming's $[7,16,3]$-code is perfect, as $\frac{2^7}{7+1} = 2^4 = |C|$. Note that if $\frac{2^n}{V(n,e)} \notin \Z$ then there is no perfect $e$-error correcting code of length $n$, and even if $2^n/V(n,e)$ is an integer is may be the case that no perfect code exists.

Define \emph{A(n,d)} $= \max\{m : \exists [n,m,d]$-code$\}$. For instance, $A(n,1) = 2^n, A(n,n) = 2, A(n,2) = 2^{n-1}$.

\begin{lemma}
$A(n,d+1) \leq A(n,d)$
\end{lemma}
\begin{proof}
Let $m = A(n,d+1)$, and pick a code $C$ with parameters $[n,m,d+1]$. Let $c_1, c_2 \in C$ with $d(c_1,c_2) = d+1$. Let $c_1'$ differ from $c_1$ in exactly one of the places where $c_1, c_2$ differ. Then $d(c_1', c_2) = d$. If $c \in C\setminus\{c_1\}$, then $d(c,c_1)\leq d(c,c_1')+d(c_1', c_1) \implies d(c_1, c_1') \geq d$.

Replacing $c_1$ by $c_1'$ gives an $[n,m,d]$-code i.e. $m \leq A(n,d)$.
\end{proof}

\begin{corollary}
Equivalently, $A(n,d) = \max\{m : \exists [n,m,d']-$code for some $d'\geq d\}$.
\end{corollary}
\begin{theorem}
\begin{align*}
\frac{2^n}{V(n,d-1)} \leq A(n,d) \leq \frac{2^n}{V(n, \floor{\frac{d-1}{2}})}
\end{align*}
The lower bound is called the Gilbert-Shannon-Varshanov (GSV) bound, whilst the upper bound follows from Hamming's bound.
\end{theorem}
\begin{proof}[Proof of GSV]
Let $m = A(n,d)$, and let $C$ be a $[n,m,d]$-code. Then there cannot exist $x \in \F_2^n$ with $d(x,c) \geq d$ for all $c \in C$, otherwise we could replace $C$ by $C \cup \{x\}$, contradicting maximality of $d(x,c)$. Hence $\F_2^n = \bigcup_{c\in C} B(c, d-1)$. Hence $2^n \leq m V(n, d-1)$.
\end{proof}
For example, take $n=10, d=3$. Then $V(n,2) = 56, V(n,1)=11$, and so these bounds give $\frac{2^10}{56} \leq A(10,3) \leq \frac{2^10}{11}$, i.e. $19\leq A(10,3)\leq 93$, but in fact we know computationally that it is between 72 and 79.

\subsection{Asymptotics}
We study $\frac{\log A(n, \floor{n\delta})}{n}$ as $n \to \infty$ to see how large the information rate can be for a given error rate.
\begin{proposition}
Let $0 < \delta < \frac12$. Then:
\begin{enumerate}
\item $\log V(n, \floor{n \delta}) \leq n H(\delta)$.
\item $\frac1n \log A(n, \floor{n\delta}) \geq 1- H(\delta)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Assuming \textit{1.} we see by the GSV bound, $A(n, \floor{n\delta}) \geq \frac{2}{V(n, \floor{n\delta}-1} \geq \frac{2^n}{V(n, \floor{n\delta})}$, so $\frac{\log A(n, \floor{n\delta})}{n} \geq 1 - \frac{\log V(n, \floor{n\delta})}{n} \geq 1- H(\delta)$, and so \textit{1.}$\implies$\textit{2.}

For \textit{1.} observe $H(\delta)$ is increasing for $\delta \leq \frac12$, so \textsc{wlog} we can assume $n\delta \in \Z$. Then:
\begin{align*}
1 &= (\delta + (1-\delta))^n\\
&= \sum_{i=0}^n \binom{n}{i} \delta^i (1-\delta)^{n-i}\\
&\geq \sum_{i=0}^{n\delta} \binom{n}{i} \delta^i (1-\delta)^{n-i}\\
&= (1-\delta)^n \sum_{i=0}^{n\delta} \binom{n}{i} \left(\frac{\delta}{1-\delta}\right)^i\\
&\geq (1-\delta)^n \sum \binom{n}{i} \left(\frac{\delta}{1-\delta}\right)^{n\delta}\\
&= \delta^{n\delta}(1-\delta)^{n(1-\delta)}V(n,n\delta)\\
0 &\geq n\delta\log\delta + n(1-\delta)\log(1-\delta)+\log V(n,n\delta)\\
0 &\geq -nH(\delta) + \log V(n,n\delta)
\end{align*}
\end{proof}
This constant $H(\delta)$ is the best possible, in the sense that:
\begin{proposition}
\begin{align*}
\lim_{n\to\infty} \frac{\log V(n, \floor{n\delta})}{n} = H(\delta)
\end{align*}
\end{proposition}
\begin{proof}
\textsc{Wlog} we may assume that $0 < \delta < \frac12$. Let $0 \leq r \leq \frac{n}{2}$. Recall that $V(n,r) = \sum_{i=0}^r \binom{n}{i}$. Then:
\begin{align*}
\binom{n}{r} \leq V(n,r) \leq (r+1)\binom{n}{r} \tag{(\ast)}
\end{align*}
From Stirling's approximation, $\log\binom{n}{r} = -r\log \frac{r}{n} - (n-r)\log\frac{n-r}{n} + \mathcal{O}(\log n) = nH(\frac{r}{n}) + \mathcal{O}(\log n)$.

Then from $(\ast)$, we have:
\begin{align*}
H(\frac{r}{n}) + \mathcal{O}\left(\frac{\log n}{r}\right) \leq \frac{\log V(n,r)}{n} \leq H\left(\frac{r}{n}\right) + \mathcal{O}\left(\frac{\log n}{n}\right)\\
\therefore \lim_{n\to\infty} \frac{\log V(n, \floor{n\delta})}{n} = H(\delta)
\end{align*}
\end{proof}
\section{New Codes from Old}
Suppose $C$ is an $[n,m,d]$-code. Then the \emph{parity check digit extension} $C^+$ of $C$ is the code of length $n+1$ given by:
\begin{align*}
C^+ = \{(c_1, \ldots, c_n, \sum_{i=1}^n c_i) : (c_1, \ldots, c_n) \in C\}
\end{align*}
where the summation is done modulo 2. It is an $[n+1, m, d']$-code where $d' = d$ or $d+1$.

We can also delete the $i\th$ digit from each codeword for $1 \leq i\leq n$, giving a \emph{truncated} or \emph{punctured} codeword (depending on if $i=n$ or $i<n$ respectively), called $C^-$, with parameters $[n-1, m, d']$ where $d-1 \leq d' \leq d$.

Finally, given some $1 \leq i \leq n, \alpha \in \F_2$, we can create the \emph{shortened} or \emph{punctured} code $C'$ of $C$ is $\{(c_1, \ldots, c_{i-1},c_{i+1}, \ldots, c_n) : (c_1, \ldots, c_{i-1}, \alpha, c_{i+1} \ldots, c_n) \in C\}$. This has parameters $[n-1, m', d']$ with $d' \geq d$ and $m' \geq \frac{m}{2}$ for $c$ a suitable choice of $\alpha$.

\section{AEP and Shannon's First Coding Theorem}
A \emph{source} is a sequence of random variables $X_1, X_2, \ldots$ taking values in some alphabet $\mathscr{A}$. A source is \emph{Bernoulli} or \emph{memoryless} if $X_1, \ldots$ are independently identically distributed (\textsc{iid}). A source $X_1, \ldots$ is \emph{reliably encodable at rate r} if there are subsets $A_n \subseteq \mathscr{A}^n$ such that:
\begin{enumerate}
\item $\lim_{n\to \infty} \frac{\log|A_n|}{n} = r$
\item $\lim_{n\to \infty} \P[(X_1, \ldots, X_n)\in A_n] = 1$
\end{enumerate}
The \emph{information rate} $H$ of a source is the infimum of all reliable encoding ratesso that $0 \leq H \leq \log |\mathscr{A}|$. Shannon's first coding theorem computes the information rate of certain sources, including Bernoulli sources.

\subsection{Reminder from 1A Probability}
A \emph{probability space} is given by a triple $(\Omega, \mathscr{F}, \P)$ where $\mathscr{F} \subset \powset(\Omega)$ is a set of \emph{events} and $\P$ is a \emph{probability measure}, and a \emph{random variable} $X$ is a function defined on $\Omega$ with some range. It has a \emph{probability mass function} $p: x \mapsto \P(X=x)$

We say that a sequence of random variables $X_1, X_2, \ldots$ \emph{converges in probability} to $\lambda \in \R$ means that
\begin{align*}
\forall\;\epsilon > 0, \lim_{n \to \infty} \P(|X_n -\lambda| \geq \epsilon) = 0
\end{align*}
We write $X_n \xrightarrow{\P} \lambda$ as $n \to \infty$.

\begin{theorem}[Weak Law Of Large Numbers, \textsc{wlln}]
Let $X_1, X_2, \ldots$ be \textsc{iid} discrete real-valued random variables with finite expected value $\mu$. Then:
\begin{align*}
\frac1n \sum_{i=1}^n X_i \xrightarrow{\P} \mu\text{ as } n\to \infty
\end{align*}
\end{theorem}
\begin{proof}
See Carne, theorem 10.3.
\end{proof}
\begin{lemma}
The information rate of a Bernoulli source $X_1, X_2, \ldots$ is at most the expected word length of an optimal code $c:\mathscr{A} \to \{0,1\}^{\ast}$ for $X_i$.
\end{lemma}
\begin{proof}
Let $\ell_1, \ell_2, \ldots$ be the lengths of codewords when we encode $X_1, X_2, \ldots$ using $c$. Then given $\epsilon >0$, let $A_n = \{x \in \mathscr{A}^n : c^{\ast}(x)$ has length $< n(\E[\ell_i] + \epsilon)\}$. Then:
\begin{align*}
\P\left[(X_1, \ldots, X_n) \in A_n\right] &= \P\left[\sum \ell_i < n(\E[\ell_i] +\epsilon)\right]\\
&\geq \P\left(\left|\frac1n \sum\ell_i - \E[\ell_i]\right| < \epsilon\right)\\
&\to 1 \text{ as } n \to \infty
\end{align*}
Now $c$ is decipherable so $c^{\ast}$ is injective, and hence $|A_n| \leq 2^{n(\E[\ell_i]+\epsilon)}$. Making $A_n$ larger if required, we may take $|A_n| =\floor{2^{n(\E[\ell_i]+\epsilon)}}$. Hence $\frac{\log|A_n|}{n} \to \E[\ell_i] + \epsilon$. So $X_1, X_2, \ldots$ is reliably encodable at a rate $r = \E[\ell_i] +\epsilon$ for any $\epsilon > 0$, and hence the information rate is at most $\E[\ell_i]$.
\end{proof}
\begin{corollary}
A Bernoulli source has information rate less than $H(X_1) + 1$.
\end{corollary}
\begin{proof}
Use \textbf{8.2} and the Noiseless Coding theorem \textbf{2.3}.
\end{proof}

Now suppose we encode $X_1, X_2, \ldots$ in blocks:
\begin{align*}
\underbrace{X_1, \ldots, X_N}_{Y_1}, \underbrace{X_{N+1}, \ldots, X_{2N}}_{Y_2}, \ldots
\end{align*}
such that $Y_1, Y_2, \ldots$ take values in $\mathscr{A}^N$. We can check that if $X_1,  X_2, \ldots$ has information rate $H$, then $Y_1, Y_2, \ldots$ has information rate $NH$.

\begin{proposition}
The information rate$H$ of a Bernoulli source $X_1, X_2, \ldots$ is at most $H(X_1)$.
\end{proposition}
\begin{proof}
Apply \textbf{8.3} to $Y_1, Y_2, \ldots$ to get:
\begin{align*}
NH < H(Y_1) + 1 = H(X_1, \ldots, X_N) + 1 = \sum_{i=1}^N H(X_i) + 1 = NH(X_i)+1
\end{align*}
i.e. $H < H(X_1) + \frac1N$ for all $N \geq 1$, and so $H \leq H(X_1)$.
\end{proof}

\hrule
\subsection{Typical Sequences}
This content is nonexaminable, but is required to prove the examinable result that $H = H(X_1)$.

\hspace*{-1em}As a motivational example, toss a biased coin with head probability $p$, and let $X_i$ be the outcome of the $i\th$ flip. If we toss a large number, say $N$, times, we expect that we will get about $pN$ heads and $(1-p)N$ tails. The probability of any particular sequence of $pN$ heads and $(1-p)N$ tails is $p^{pN}(1-p)^{(1-p)N} = 2^{-NH(X)}$.

We say that a source $X_1, X_2, \ldots$ satisfies the \emph{Asymptotic Equipartition Property (AEP)} for some constant $H \geq 0$ if:
\begin{align*}
-\frac1n\log p(X_1, \ldots, X_n) \to H \text{ as } n \to \infty
\end{align*}
\begin{lemma}
The AEP for a source $X_1, x_2, \ldots$ is equivalent to the following:\\
$\forall\;\epsilon>0 \exists n_0(\epsilon) \st \forall\;n\geq n_0(\epsilon) \exists T_n \subseteq \mathscr{A}^n \st$
\begin{itemize}
\item $P[(X_1, \ldots, X_n) \in T_n] > 1-\epsilon$
\item $\forall\;(x_1, \ldots, x_n) \in T_n, 2^{-n(H+\epsilon)} \leq p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)}$
\end{itemize}
The $T_n$ are called \emph{typical sets}, and the $(x_1, \ldots, x_n) \in T_n$ are \emph{typical sequences}.
\end{lemma}
\begin{proof}
If $(x_1, \ldots, x_n) \in \mathscr{A}^n$ then we have the following equivalence:
\begin{align*}
2^{-n(H+\epsilon)} \leq p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)} \iff |-\frac1n \log p(x_1, \ldots, x_n) - H| \leq \epsilon \tag{\dagger}
\end{align*}
Both AEP and the claimed equivalent results say that $P((X_1, \ldots, X_N)\text{ satisfies } \dagger) \to 1$ as $n \to \infty$.
\end{proof}

\begin{theorem}[Shannon's First Coding Theorem]
If a source $X_1, X_2, \ldots$ satisfies the AEP with constant $H$ then it has information rate $H$.
\end{theorem}
\begin{proof}
Let $\epsilon >0$ and let $T_n \subseteq \mathscr{A}^n$ be typical sets. Then for all $(x_1, \ldots, x_n) \in T_n$:
\begin{align*}
p(x_1, \ldots, x_n) \geq 2^{-n(H+\epsilon)} \implies 1 \geq |T_n|2^{-n(H+\epsilon)} \implies \frac{\log|T_n|}{n} \leq (H+\epsilon)
\end{align*}
Taking $A_n = T_n$ in the definition of reliable encoding, we see that the source is reliably encodeable at rate $H+\epsilon$. As $\epsilon >0$, the information rate is $\leq H$.

Conversely, if $H=0$ we're done, otherwise pick $0 <\epsilon < \frac{H}{2}$. Suppose for a contradiction that the source is reliably encodeable at rate $H-2\epsilon$, say, with sets $A_n \subseteq \mathscr{A}^n$. Let $T_n \subseteq \mathscr{A}^n$ be typical sets. Then for all $(x_1, \ldots, x_n) \in T_n, p(x_1, \ldots, x_n) \leq 2^{-n(H-\epsilon)}$

Hence $\P(A_n \cap T_n) \leq 2^{-n(H-\epsilon)}$, and so $\frac{\log\P(A_n\cap T_n)}{n} \leq (H-\epsilon) + \frac{\log |A_n|}{n}\xrightarrow{n \to \infty} -\epsilon$. So $\P(A_n \cap T_n) \to 0$ as $n \to \infty$. But $\P(T_n) \leq \P(T_n\cap A_n) + \P(\mathscr{A}^n \setminus A_n) \to 0 + 0 = 0$ as $n \to \infty$, contradicting typicality of $T_n$. Hence the source cannot be reliably encoded at rate $H-2\epsilon$, and so the information rate must be $\geq H$, and hence $=H$.
\end{proof}
\section{Capacity and Shannon's Second Coding Theorem}
Given a random variable $X$ with mass function $p_X$, we can construct a new random variable $p(X) = p_X \circ X$, taking values in $[0,1]$. Then $H(X) = \E(-\log p(X))$. For example, if $X,Y$ are independent, then $p(X,Y) = p(X)p(Y)$, and so $-\log p(X,Y) = -\log p(X) - \log p(Y) \implies H(X,Y) = H(X)+H(Y)$.

\begin{corollary}
A Bernoulli source $X_1, X_2, \ldots$ has information rate $H(X_1) = H$.
\end{corollary}
\begin{proof}[Proof*]
$p(X_1, \ldots, X_n) = p(X_1)\ldots p(X_n)$, and hence we have:
\begin{align*}
-\frac{\log p(X_1, \ldots X_n)}{n} = -\frac{1}{n}\sum_{i=1}^n \log p(X_i) \xrightarrow{\P} H(X_i)
\end{align*}
by the weak law of large numbers, and using the fact that the $X_i$ are i.i.d. Check as an exercise that the AE holds with constant $H(X_1)$ using the definition of convergence in probability. Hence by \textbf{8.6} we are done.
\end{proof}
Note that \textbf{8.4} gave us an information rate $\leq H(X_1)$, without the use of the AEP. The AEP can also be used for noiseless coding - we encode the typical sequences with a block code and the atypical sequences arbitrarily, since they rarely occur. Many sources of interest, not just Bernoulli sources, satisfy the AEP. Under suitable conditions, the sequence $\frac1n H(X_1, \ldots, X_n)$ is decreasing and the AEP is satisfied with constant $H \lim_{n\to \infty} \frac{H(X_1,\ldots, X_n}{n}$.

Consider a communication channel with input of alphabet $\mathscr{A}$, and output $B$. A code of length $n$ is a subset $C \subseteq \mathscr{A}^n$. The \emph{error rate}
\begin{align*}
\hat{e}(C) = \max \P(\text{error }|c\text{ sent})
\end{align*}

The \emph{information rate} is $\rho(C) = \frac{\log |C|}{n}$.

The channel can \emph{transmit reliably} at rate $R$ if there are codes $C_1, C_2, \ldots$ with $C_n$ of length $n$, and:
\begin{itemize}
\item $\lim_{n \to \infty} \rho(C_n) = R$.
\item $\lim_{n \to \infty} \hat{e}(C_n) = 0$.
\end{itemize}
The \emph{operational capacity} is the supremum of all reliable transmission rates.

Assume a source has information rate $r$ bits per symbol, and emits symbols at $s$ symbols per second, whilst the channel has capacity $R$ bits per transmission and can can transmit symbols at $S$ transmissions per second. Usually $S=s=1$. If $rs \leq RS$ then we can encode and transmit reliably, and if $rs > RS$ we cannot.

\begin{proposition}
A binary symmetric channel with error probability $p < \frac{1}{4}$ has non-zero capacity.
\end{proposition}
\begin{proof}
We use the $GSV$ bound. Pick $\delta$ with $2p <\delta  < \frac12$. We claim reliable transmission rate of $R = 1-H(\delta) > 0$.

Let $C_n$ be a code of length $n$ with minimum distance $\floor{n\delta}$ of maximal size. Them $|C_n| = A(n, \floor{n\delta}) \geq 2^{n(1-H(\delta)} = 2^{nR}$.

Replacing $C_n$ by a subcode we can assume $|C_n| = \floor{2^{nR}}$ with minimum distance still $\geq \floor{n\delta}$

Now, with minimum distance decoding, $\hat{e}(C_n) \leq \P($in $n$ uses the BSC makes more than $\frac{n\delta-1}{2}$ errors$)$.

Pick $\epsilon > 0$ with $p+\epsilon < \frac{\delta}{2}$. For $n$ sufficiently large we have that $\frac{n\delta-1}{2} = n(\frac{\delta}{2} - \frac{1}{2n}) > n(p+\epsilon)$.

Hence $\hat{e}(C) \leq \P($BSC makes more than $n(p+\epsilon)$ errors$) \to 0$, as we will see in the next lemma.
\end{proof}
\begin{lemma}
Let $\epsilon > 0$. A BSC with error probability $p$ is used to transmit $n$ digits. Then:
\begin{align*}
\lim_{n \to \infty} \P(\text{BSC makes at least }n(p+\epsilon)\text{ errors}) = 0
\end{align*}
\end{lemma}
\begin{proof}
If $U_i$ is the Bernoulli random variable that digit $i$ is mistransmitted. Then $U_i$ are i.i.d. with probability $p$. So $\E[U_i] = p$. Then the probability we are interested in is $\P(\sum U_i \geq n(p+\epsilon)) \leq \P(|\frac{1}{n}\sum U_i -p| \geq \epsilon) \to 0$ by the \textsc{wlln}.
\end{proof}

\subsection{Fano's Inequality}
Let $X,Y$ be random variables taking values in the alphabets $\mathscr{A}, \mathscr{B}$. Then:
\begin{itemize}
\item $H(X|Y=y) = -\sum_{x \in \mathscr{A}} \P(X=x|Y=y)\log \P(X=x|Y=y)$
\item $H(X|Y) = \sum_{y \in \mathscr{B}} \P(Y=y)H(X|Y=y)$
\end{itemize}
Clearly $H(X|Y) \geq 0$.
\begin{lemma}
\begin{align*}
H(X,Y) = H(X|Y) + H(Y)
\end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
H(X|Y) &= -\sum_{x\in\mathscr{A}} \sum_{y \in \mathscr{B}} \P(X=x|Y=y)\P(Y=y)\log \P(X-x|Y=y)\\
&= -\sum_{x\in\mathscr{A}} \P(X=x, Y=y) \log \frac{\P(X=x, Y=y}{\P(Y=y)}\\
&= -\sum_{(x,y)\in\mathscr{A}\times\mathscr{B}}\underbrace{\P(X=x,Y=y)\log\P(X=x,Y=y)}_{H(X,Y)} + \underbrace{\P(X=x, Y=y)\log\P(Y=y)}_{-H(Y)}\\
&= H(X,Y) - H(Y)
\end{align*}
\end{proof}
\begin{corollary}
$H(X|Y) \leq H(X)$ with equality if and only if $X,Y$ are independent.
\end{corollary}
\begin{proof}
Combine \textbf{4.1} with \textbf{9.4}.
\end{proof}
We can replace $X,Y$ by random variables $X_1, X_2, \ldots, X_r$ and $Y_1, Y_2, \ldots, Y_s$, and similarly define $H(X_1, \ldots, X_r|Y_1, \ldots, Y_s)$\footnote{$H(X,Y|Z)$ means ``the entropy of ($X$ and $Y$) given $Z$", \textsc{not} ``the entropy of $X$ and ($Y$ given $Z$)"}.

\begin{lemma}
\begin{align*}
H(X|Y) \leq H(X|Y, Z) + H(Z)
\end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
H(X,Y,Z) &= H(Z|X,Y) + H(X,Y) = H(Z | X, Y) + H(X|Y) + H(Y)\\
H(X,Y,Z) &= H(X|Y,Z) + H(Y,Z) = H(X|Y, Z) + H(Z|Y) + H(Y)\\
\therefore H(X|Y) &= -H(Z|X,Y) + H(X|Y,Z) + H(Z|Y)\\
&\leq H(X|Y,Z) + H(Z) 
\end{align*}
\end{proof}
\begin{proposition}[Fano's Inequality]
Let $X,Y$ be random variables taking values in $\mathscr{A}$, where $|\mathscr{A}| = m$. Let $p = \P(X\neq Y)$. Then:
\begin{align*}
H(X|Y) \leq H(p) + p \log(m-1)
\end{align*}
\end{proposition}
\begin{proof}
Let $Z = \begin{cases} 0 & X = Y \\ 1 & X \neq Y\end{cases}$. Then $\P(Z=0) = 1-p, \P(Z=1) = p$. So $H(Z) = H(p)$. By \textbf{9.6},
\begin{align*}
H(X|Y) \leq H(p) + H(X|Y,Z)\tag{\ast}
\end{align*}
Then we have two cases:
\begin{itemize}
\item[$Z=0$:] We must have $X=y$, so $H(X|Y=y, Z=0) = 0$.
\item[$Z=1$:] Just $m-1$ remaining possibilities for $X$, so $H(X|Y=y, Z=1) \leq \log(m-1)$.
\end{itemize}
Hence we have:
\begin{align*}
H(X|Y,Z) &= \sum_{y,z} \P(Y=y, Z=z)H(X|Y=y,Z=z)\\
&\leq \sum_y \P(Y=y, Z=1)\log(m-1)\\
&= \P(Z=1)\log(m-1) = p\log(m-1)
\end{align*}
Then by $(\ast)$, $H(X|Y) \leq H(p) + p \log(m-1)$.
\end{proof}
We will apply this result later when $X$ takes values in $\mathscr{A}$ and $Y$ is the result of passing codewords through a channel and then decoding, where $p$ will be the probability of error.

If $X,Y$ are random variables, then we define the \emph{mutual information} of $X$ and $Y$ to be:
\begin{align*}
I(X;Y) \coloneqq H(X) - H(X|Y)
\end{align*}
By \textbf{4.1} and \textbf{9.4}, $I(X;Y) = H(X) + H(Y) - H(X,Y) \geq 0$, with equality if and only if $X$ and $Y$ are independent. We also see from this form that $I(X;Y) = I(Y;X)$.

Given a DMC with input alphabet $\mathscr{A}$ of size $m$, and output alphabet $\mathscr{B}$. Let $X$ be a random variable taking values in $\mathscr{A}$ used as an input to the channel. Let $Y$ be the random variable output, depending on both $X$ and the channel matrix.

We define the \emph{information capacity} to be the $\max_X I(X;Y)$, where the maximum is taken over all probability distributions $(p_1, \ldots, p_m)$ for $X$. Since the space of random variables for $X$ is a closed and bounded subset of $\R^m$, by Heine-Borel it is compact and, since $I$ is continuous, the maximum is attained. Note that the information capacity depends only on the channel matrix.

\begin{theorem}[Shannon's Second Coding Theorem]
For a DMC, the operational capacity is equal to the information capacity.
\end{theorem}
We will prove $\leq$ in general, and $\geq$ for a binary symmetric channel only. 

For example, with a BSC with error probability $p$, input $X$, output $Y$, then $\P(X=0) = \alpha, \P(X=1) = 1-\alpha$, and so $\P(Y=0) = \alpha(1-p) + (1-\alpha)p; \P(Y=1) = (1-\alpha)(1-p) + \alpha p$.

Then $C = \max_{\alpha} I(X;Y) = \max_\alpha \left[H(\alpha(1-p) + (1-\alpha)p) - H(p)\right] = 1-H(p$, where the max attained at $\alpha = \frac12$. So the information capacity $C = 1+p\log p + (1-p)\log(1-p)$. We can plot this on a graph:
\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[ymin=0, ymax=1, xmin=0, xmax=1]
\addplot[samples=200, blue, ultra thick] (x, {1+x*log2(x) + (1-x)*log2(1-x)});
\end{axis}
\end{tikzpicture}
\caption{Capacity as a function of $p$ for a binary symmetric channel}
\end{figure}
At $p=0,1$ the channel transmits perfectly, whilst at $p=\frac{1}{2}$ no information can be transmitted. We can choose to calculate $H(Y)-H(Y|X)$ or $H(X)-H(X|Y)$ to find the information - often one is easier than the other, for example with the binary erasure channel with erasure probability $p$.

$\P(X=0) = \alpha, \P(X=1) = 1-\alpha, \P(Y = 0) = \alpha(1-p), \P(Y=\ast) = p, \P(Y=1) =(1-\alpha)(1-p)$

Then $H(X|Y=0) = 0; H(X|Y = \ast) = H(\alpha); H(X|Y = 1) = 0$, and so $H(X|Y) = pH(\alpha)$. So $C = \max_\alpha (H(\alpha) - pH(\alpha)) = 1-p$, where the maximum value is attained for $\alpha =\frac12$.
\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[ymin=0, ymax=1, xmin=0, xmax=1]
\addplot[samples=200, blue, ultra thick] (x, {1-x});
\end{axis}
\end{tikzpicture}
\caption{Capacity as a function of $p$ for a binary erasure channel}
\end{figure}

We can now model using a channel $n$ times, called the \emph{n\th extension}, i.e. replace input and output alphabets $\mathscr{A}, \mathscr{B}$ by $\mathscr{A}^n, \mathscr{B}^n$.

\begin{lemma}
The $n\th$ extension of a discrete memoryless channel with information capacity $C$ has information capacity $nC$.
\end{lemma}
\begin{proof}
The input $X_1, \ldots, X_n$ determines the output $Y_1, \ldots, Y_n$, and since the channel is memoryless, $H(Y_1,\ldots,Y_n|X_1,\ldots,X_n) = \sum_i H(Y_i|X_1, \ldots, X_n) = \sum_i H(Y_i|X_i)$.
\begin{align*}
I(X_1, \ldots, X_n; Y_1, \ldots, Y_n) &= H(Y_1, \ldots, Y_n) - H(Y_1, \ldots, Y_n|X_1, \ldots, X_n)\\
&= H(Y_1, \ldots, Y_n) - \sum_{i=1}^n H(Y_i|X_i)\\
&\leq \sum_{i=1}^n \left[ H(Y_i) - H(Y_i|X_i)\right]\\
&= \sum_{i=1}^n I(X_i; Y_i) \leq nC
\end{align*}
To finish, we must find $X_1, \ldots X_n$ giving equality - take $X_i$ to be i.i.d. with $I(X_i, Y_i) = C$, giving equality at the second $\leq$. Then the $Y_i$ are i.i.d. so we have equality at the first $\leq$. So the maximum possible value is $nC$.
\end{proof}

\begin{proposition}
For a DMC the operational capacity is at most the information capacity.
\end{proposition}
\begin{proof}
Let $C$ be the information capacity. Suppose that reliable transmission is possible at some $R>C$, i.e. there is a sequence $C_1, C_2, \ldots$, with $C_n$ of length $n$ such that $\lim_{n\to \infty} \rho(C_n) = R$, and $\lim_{n\to\infty} \hat{e}(C_n) = 0$.

We define the \emph{average error rate} $e(C_n) = \frac{1}{|C_n|}\sum_{c \in C_n}\P(\text{error}|c\text{ sent})$, so that $e(C_n) \leq \hat{e}(C_n)$. Hence $e(C_n) \to 0$ as $n \to \infty$. Then let $X$ be a random variable equidistributed in $C_n$. Transmit $X$ and decode to obtain $Y$. So $e(C_n) = \P(X\neq Y)$. Then $H(X) = \log|C_n| = \log\floor{2^{nR}} \geq nR -1$ for sufficiently large $n$. By Fano's inequality,
\begin{align*}
H(X|Y) \leq 1 +e(C_n)\log(|C_n|-1) \leq 1 + e(C_n)n\rho(C_n)
\end{align*}
By the previous proposition,
\begin{align*}
nC \geq I(X,Y) &= H(X) - H(X|Y) \\
&\geq \log|C_n| - (1+e(C_n)n\rho(C_n))\\
&= n\rho(C_n) - e(C_n)n\rho(C_n) - 1\\
\therefore e(C_n)n\rho(C_n) &\geq n(\rho(C_n) - C)-1\\
e(C_n) &\geq \frac{\rho(C_n) - C}{\rho(C_n)} - \frac{1}{n\rho(C_n)} \to \frac{R-C}{R}
\end{align*}
But $R>C$, so $e(C_n) \not\to 0$ as $n \to \infty$. \contr
\end{proof}

\begin{proposition}
Consider the Binary Symmetric Channel with error probability $p$. Let $R < 1- H(p)$. Then there exists a sequence of codes $C_1, C_2, \ldots$, with $C_n$ of length $n$ and size $\floor{2^{nR}}$, such that:
\begin{align*}
\lim_{n \to \infty} \rho(C_n) &= R\\
\lim_{n \to \infty} e(C_n) &= 0
\end{align*}
Note this is $e(C_n)$, not $\hat{e}(C_n)$.
\end{proposition}
\begin{proof}
We use the method of random codes.

Without loss of generality, assume $p <\frac12$. Let $\epsilon > 0$ be such that $p + \epsilon < \frac12$ and $R < 1 - H(p + \epsilon)$. This is always possible since $H$ is continuous. Let $m = \floor{2^{nR}}$ and pick $C \in \mathscr{C} = \{[n,m]-$binary codes$\}$ and random. Note $|\mathscr{C}| = \binom{2^n}{m}$. Let $\mathscr{X}$ be a random variable equidistributed throughout $\mathscr{C}$, say $\mathscr{X} = \{X_1, \ldots, X_m\}$ where the $X_i$ are random variables taking values in $\F_2^n$, such that:
\begin{align*}
\P(X_i = x_i | \mathscr{X} = C) = \begin{cases} \frac1m & x_i \in C \\ 0 & \ow \end{cases}
\end{align*}
Notice that $\P(X_2 = x_2| X_1 = x_1) = \begin{cases} \frac{1}{2^n-1} & x_1\neq x_2\\ 0 & \ow \end{cases}$

Send $X = X_1$ through the BSC, receive $Y$, and decode to obtain $Z$. Under the minimum distance decoding, $\P(X \neq Z) = \frac{1}{|\mathscr{C}|}\sum_{C \in \mathscr{C}} e(C)$.

It then suffices to show that $\P(X \neq Z) \to 0$ as $n \to \infty$. Let $r = \floor{n(p+2)}$.
\begin{align*}
\P(X \neq Z) &\leq \P(B(Y,r) \cap \mathscr{X} \neq \{X\})\\
&= \underbrace{\P(X \notin B(Y,r))}_{(i)} + \underbrace{\P(B(Y,r) \cap \mathscr{X} \supsetneq \{X\})}_{(ii)}
\end{align*}
\begin{itemize}
\item[$(i)$:] $\P(X \notin B(Y,r)) = \P($BSC makes more than $r$ errors$) \to 0$ as $n \to \infty$ by \textbf{9.3}.
\item[$(ii)$:] \begin{align*}
\P(B(Y,r) \cap \mathscr{X} \supsetneq \{X\}) &\leq \sum_{i=2}^m \P(X_i \in B(Y,r)\text{ and }X_1 \in B(Y,r))\\
&\leq \sum_{i=2}^m \P(X_i \in B(Y,r) | X_1 \in B(Y,r))\\
&= (m-1)\frac{V(n,r)-1}{2^n-1}\\
&\leq m\frac{V(n,r)}{2^n}\\
&\leq 2^{nR}2^{nH(p+\epsilon)}2^{-n} = 2^{n(R-(1-H(p+\epsilon)))} \to 0
\end{align*}
since $R < 1-H(p+\epsilon)$
\end{itemize}
\end{proof}
\begin{proposition}
We can replace $e$ by $\hat{e}$ in the previous proposition.
\end{proposition}
\begin{proposition}
Pick $R'$ such that $R<R'<1-H(p)$. Then the previous proposition constructs $C_1', C_2', \ldots$, with $C_n'$ of length $n$, size $\floor{2^{nR'}}$, and $e(C_n') \to 0$ as $n \to \infty$.

Order then codewords of $C_n'$ by $\P($error$|c\sent)$, and delete the worse half them to give $C_n$. Then we have $|C_n| = \floor{\frac{|C_n'|-1}{2}}, \hat{e}(C_n) \leq 2e(C_n')$. Then $\rho(C_n) \to R$ and $\hat{e}(C_n) \to 0$ as $n \to \infty$.
\end{proposition}
Note that:
\begin{enumerate}
\item \textbf{9.12} says we can transmit reliably at any rate $R < 1-H(p)$ so the capacity is at least $1-H(p)$. But, by \textbf{9.10}, the capacity at most $1-H(p)$, and hence the BSC with error probability has capacity $1-H(p)$.

\item The proof shows that good codes exist, but it does not tell us how to find them.
\end{enumerate}
\section{Interlude: An Application to Gambling}
Let $0 < p < 1$, $n>0$, and $0 \leq w <1$. A coin is tossed $n$ times in succession. $P(H) = p$ and, if I pay $k$ ahead of a particular throw, then I get back $kn$ if the throw is head, but nothing if the throw is a tail.

What is my strategy?
\begin{itemize}
\item If $pn <1$, then don't bet - your expected winnings is $<0$.
\item If $pn >1$, then we want to bet, but how much? A larger bet means more winnings, but also more risk. What proportion $w$ of our total wealth should we bet?
\end{itemize}

Note that $w$ is always the same, only the size of the fortune changes. Let the fortune by $X_j$ after the $j\th$ throw. I bet $wX_j$, retaining $(1-w)X_j$. My fortune $X_{j+1}$ after the $(j+1)\th$ throw is:
\begin{align*}
X_{j+1} = \begin{cases} X_j(Xn + 1-w) & j\th \text{ throw is H}\\X_j(1-w) & j\th\text{ throw is T}\end{cases}
\end{align*}
Put $Y_{j+1} = \frac{X_{j+1}}{X_j} = \begin{cases} wn + (1-2) & H \\ 1-w & T\end{cases}$.

Then we try to maximise the log of our fortune: let $\E\log Y_i = \bar{\mu}4, \Var(\log Y_i) = \bar{\sigma}^2$. If $a > 0$ then:
\begin{align*}
\P\left(\left| \frac{\log Y_1+\ldots + \log Y_n}{n} - \bar{\mu}\right|\geq a\right) & \leq \frac{\bar{\sigma}^2}{na^2}
\end{align*}
by Chebyshev's inequality. But $\sum \log Y_i = \log X_i$, so we have:
\begin{align*}
\P\left(\left| \frac{\log X_n}{n} - \bar{\mu}\right|\geq a\right) & \leq \frac{\bar{\sigma}^2}{na^2}
\end{align*}
i.e., for all $\epsilon > 0$, $\delta > 0$, there is some $N$ such that
\begin{align*}
\P\left(\left| n^{-1}\log(X_n) - \bar{\mu}\right| \geq \delta\right) \leq \epsilon\;\;\forall\;\N
\end{align*}
\begin{lemma}
Consider one single toss of a coin with $\P(H) = p < 1$. Suppose that a bet on heads has payout ratio of $n$. Suppose that we have a bankroll of $1$ unit and we bet $w$ on $H$, retaining $1-w$ for $0\leq 1 \leq 1$. If $Y$ is the expected value of our fortune after the throw then
\begin{align*}
\E(\log Y) = p\log(1+(n-1)w) + (1-p)\log(1-w)
\end{align*}
The value of $\E(\log Y)$ is maximised by taking $w = 0$ if $np \leq 1$ and setting $w = \frac{np-1}{n-1}$ if $np > 1$.
\end{lemma}
A better who maximises the $\log$ of his or her fortune is called a \emph{Kelly better}, and is following \emph{Kelly's rule}, since Kelly in 1956 showed how to do this using information theory.

Shannon's Second Coding theorem says that information can be transmitted over a channel at a rate close to the capacity with negligible error rate, provided that we're allowed arbitrarily long messages. Kelly's rule says that a gambler can, with high probability, increase her fortune at a certain optimum rate, provided that she can continue to bet for long enough.

\section{Algebraic Coding Theory}
\subsection{Linear Codes}
A code $C \in \F_2^n$ is \emph{linear} if $0 \in C$ and $x, y \in C \implies x+y \in C$. I.e., $C$ is an $\F_2$-vector subspace of $\F_2^n$. The \emph{rank} of $C$ is its dimension as an $\F_2$-vector subspace. A linear code of length $n$, rank $k$ is denoted an \emph{(n,k)-code}, using round brackets instead of square brackets. If its minimum distance is $d$, it is an $(n,k,d)$-code.

Let $v_1, \ldots, v_k$ be a basis for $C$. Then $C = \left\{\sum_{i=1}^k \lambda_i v_i : \lambda_i \in \F_2\right\}$, so $|C| = 2^k$, and $(n,k)$-codes are $[n,2^k]$-codes. The information rate is then $\frac{k}{n}$.

The \emph{weight} of $x \in \F_2^n$, denoted $w(x) = d(x,0)$.

\begin{lemma}
The minimum distance of a linear code is the minimum weight of a non-zero codeword.
\end{lemma}
\begin{proof}
Suppose $d(x,y) = d$. Then $d(x+y, y+y) = d(x+y,0) = w(x+y) = d$, and $x+y \in C$ as $C$ is linear.
\end{proof}

For $x, y \in \F_2^n$, let $x\cdot y = \sum_{i=1}^n x_iy_i \in \F_2$. Note that there are $x \neq 0$ with $x\cdot x = 0$. We then define the \emph{parity check code} defined by $P \subseteq \F_2^n$ to be $C = \{x \in \F_2^n : p\cdot x = 0 \;\forall p\in P\}$.

We've already seen some examples of this:
\begin{itemize}
\item $P = \{111\ldots 1\}$ gives the simple parity check code.
\item $P = \{1010101,0110011,0001111\}$ gives Hamming's original code.
\end{itemize}
\begin{lemma}
Every parity check code is linear.
\end{lemma}
\begin{proof}
$0 \in C$ as $p\cdot 0 = 0\;\forall p \in P$. If $x, y \in C$, then $p\cdot(x+y)=p\cdot x+p\cdot y = 0+0 = 0$, so $x+y \in P$,
\end{proof}

If $C \subset \F_2^n$ is a linear code, then the \emph{dual code} of $C$, $C^{\bot} = \{x\in\F_2^n: x\cdot y =0 \;\forall y \in C\}$.

This is a parity check code so is linear. Note that $C \cap C^{\bot}$ can be larger than $\{0\}$.

\begin{lemma}
$\rank(C) + \rank(C^{\bot}) = n$
\end{lemma}
\begin{proof}
Linear algebra using $C^{\bot}$ is the annihilator of $C$, or see later.
\end{proof}
\begin{lemma}
If $C$ is linear, then $(C^{\bot})^{\bot} = C$, and in particular $C$ is a parity check code.
\end{lemma}
\begin{proof}
If $x \in C$ then $x \cdot y = 0$ for all $y \in C^{\bot}$. Hence $x \in (C^{\bot})^{\bot}$, and so $C \subseteq (C^{\bot})^{\bot}$.

Then $\rank(C) = n - \rank(C^{\bot}) = n-(n-\rank((C^{\bot})^{\bot})) = \rank(C^{\bot})^{\bot})$, and so $C = (C^{\bot})^\bot$.
\end{proof}

Let $C$ be a $(n,k)$-code. Then we have a couple of useful matrices:
\begin{enumerate}
\item A \emph{generator matrix} $G$ for $C$ is a $k\times n$ matrix with rows a basis of $C$.
\item A \emph{parity check matrix} $H$ for $C$ is a generator matrix for $C^\bot$.
\end{enumerate}
The codewords in $C$ can be viewed as linear combinations of row of $G$, or as linear dependence relations between the columns of $H$.

\subsection{Syndrome Decoding}
If $C$ is an $(n,k)$linear code, then the syndrome of $x \in \F^n_2$ is $Hx$. If we receive $x = c+z$ where $c$ is a codeword and $z$ is an error pattern, then $Hx = Hc+Hz = Hz$.

If $C$ is $e$-error correcting, we pre-compute $Hz$ for all $z$ with $w(z) \leq e$. On receiving $x \in \F_2^n$ we look for $Hx$ in our list. Then if $Hx = Hz$, $H(x-z) = 0$, so $c = x-z \in C$ with $d(x,c) \leq e$.

Codes $C_1, C_2 \in \F_2^n$ are \emph{equivalent} if, reordering each codeword of $C_1$ using the same permutation each time, gives the codewords of $C_2$. Usually we only consider codes up to equivalence.

\begin{proposition}
Every $(n,k)$-linear code is equivalent to one with generator matrix of the form $G = (I_k | B)$ for some $k \times (n-k)$ matrix $B$.
\end{proposition}
\begin{proof}
Using elementary row operations we can transform $G$ into row echelon form
\begin{align*}
G_{ij} = \begin{cases} 0 & j < \ell(i)\\1 & j = \ell(i) \end{cases}
\end{align*}
for some $\ell(1) <\ell(2) < \ldots < \ell(k)$. Permuting the columns replaces the code by an equivalent code, and so \textsc{wlog} we can assume $\ell(i) = i$ for all $i$. Then we can cancel all the non-zero off diagonal entries on the left hand square of $G$, to get it in the form $(I_k|B)$.
\end{proof}
If we have a message $y \in \F_2^k$ viewed as a row matrix, we encode it as $yG$. If $G$ is in the form of \textbf{11.5}, then $yG = (y|yB)$ where $y$ is the message and $yB$ are check digits. Any code whose codewords can be split up in this manner is called \emph{systematic}. This form of the generator matrix will let us prove \textbf{11.3} without working with dual spaces:
\begin{proof}[Proof of 11.3]
Without any loss of generality, we may take $C$ to have generator matrix $G = (I_k|B)$. $G$ has $k$ linearly independent columns, so the linear map $c:\F_2^n \to \F_2^k; x \mapsto Gx$ is surjective with kernel $C^{\bot}$. By the rank nullity, $\dim \F_2^n = \rank C + \rank C^{\bot}$.
\end{proof}
\begin{lemma}
An $(n,k)$-linear code with generator matrix $G = (I_k |B)$ has parity check matrix $H = (B^t |I_{n-k})$.
\end{lemma}
\begin{proof}
$GH^t = (I_k |B)\left(\frac{B}{I_{n-k}}\right) = B+B = 0$, so the rows of $H$ generate a subcode of $C^{\bot}$. But the rank of $H = n-k$since $H$ contains $I_{n-k}$ as a submatrix and $n-k = \rank C^{\bot}$, so $C^{\bot}$ has generator matrix $H$, and $H$ is a parity check matrix for $C$.
\end{proof}
\section{Examples of Codes}
\subsection{Hamming Codes}
For $d \geq 1$ let $n = 2^d - 1$. Let $H$ be the $d \times n$ matrix whose columns are the non zero elements of $\F_2^d$. The \emph{Hamming (n,n-d)-code} is the linear code with parity check matrix $H$. For example with $d = 3$:
\begin{align*}
H = \begin{pmatrix} 1&0&1&0&1&0&1\\0&1&1&0&0&1&1\\0&0&0&1&1&1&1\end{pmatrix}
\end{align*}
\begin{lemma}
The minimum distance of the $(n,n-d)$-Hamming code $C$ is $d(C) = 3$. It is a perfect 1-error correcting code.
\end{lemma}
\begin{proof}
The codewords of $C$ are dependence relations between the columns of $H$. Any two columns of $H$ are linearly independent, so there are no codewords of weight at most 2. Hence $d(C) \geq 3$. Noting that $11100\ldots 0 \in C$, we have $d(C) = 3$. It is 1-error correcting by \textbf{5.3}. Moreover, $\frac{2^n}{V(n,1)} = \frac{2^n}{n+1} = 2^{n-d} = |C|$, so $C$ is perfect.
\end{proof}
\subsection{Reed-Muller Codes}
Take some finite set $X$ of size $n$ say. Then there is a correspondence between $\powset(X)$ and $\F_2^n$ given by indicator functions. Moreover, we have that the following set-theoretic operations in $\powset(X)$ correspond to vector operations on $\F_2^n$:
\begin{align*}
A \Delta B = (A \setminus B) \cup (B \setminus A) &\xleftrightarrow{\hspace{2cm}} x+y = (x_1+y_1,\ldots, x_n+y_n)\\
A \cap B &\xleftrightarrow{\hspace{2cm}} x \wedge y = (x_1y_1, \ldots, x_ny_n)
\end{align*}
Take $X = F_2^d$, so $n = 2^d$. Let $v_0 = \one_X = (1,1,\ldots, 1)$. Let $v_i = \one_{H_i}$ for $1 \leq i \leq d$, where $H_i = \{p \in X : p_i = 0\}$, the coordinate hyperplane.

Then the \emph{Reed-Muller code RM(d,r)} of order $r$ where $0\leq r\leq d$ of length $2^d$ si the linear code spanned by $v_0$ and all wedge products of $r$ or fewer of the $v_i$. By convention, the empty wedge is $v_0$. For example, with $d = 3, X = \F_2^3$ in binary number order:
\begin{center}
\begin{tabular}{c|cccccccc}
$X$&000&001&010&011&100&101&110&111\\\hline
$v_0$&1&1&1&1&1&1&1&1\\
$v_1$&1&1&1&1&0&0&0&0\\
$v_2$&1&1&0&0&1&1&0&0\\
$v_3$&1&0&1&0&1&0&1&0\\
$v_1\wedge v_2$&1&1&0&0&0&0&0&0\\
$v_2\wedge v_3$&1&0&0&0&1&0&0&0\\
$v_1\wedge v_3$&1&0&1&0&0&0&0&0\\
$v_1\wedge v_2\wedge v_3$&1&0&0&0&0&0&0&0
\end{tabular}
\end{center}
Note that:
\begin{itemize}
\item RM(3,0): spanned by $v_0$, is a repetition code of length 8.
\item RM(3,1): spanned by $v_0,v_1,v_2,v_3$, is a parity check extension of Hamming's original code.
\item RM(3,2): an $(8,7)$-code, which is actually a simple parity check code.
\item RM(3,3): the trivial code for $\F_2^8$.
\end{itemize}
\begin{theorem}
\item
\begin{enumerate}
\item The vectors $v_{i_0} \wedge \ldots \wedge v_{i_j}$ form a basis for $\F_2^n$.
\item The rank of $RM(d,r) = \sum_{s=0}^r \binom{d}{s}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\item
\begin{enumerate}
\item We've listed $\sum_{s=0}^d \binom{d}{s} = (1+1)^d = 2^d$ vectors, so it's enough to check spanning, i.e. that $RM(d,d) = \F_2^n$. Let $p \in X$. Put $y_i = \begin{cases} v_i & p_i=0\\ v_0+v_i & p_i=1\end{cases}$. Then $\one_{\{p\}} = y_1 \wedge \ldots \wedge y_d$, and we can expand this using distributivity to get $\one_{\{p\}} \in RM(d,d)$. But $\{\one_{\{p\}} : p \in X\}$ spans $\F_2^n$, and so the given vectors form a basis.
\item $RM(d,r)$ is spanned by $v_{i_1} \wedge \ldots \wedge v_{i_s}$ for $1 \leq i < \ldots <i_s \leq d$ with $0 \leq s \leq r$. These vectors are linearly independent by part \textit{1.}, and so form a basis. Hence $\rank RM(d, r) = \sum_{s=0}^r \binom{d}{s}$.
\end{enumerate}
\end{proof}
\end{document}